{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beginning-lebanon",
   "metadata": {},
   "source": [
    "# 4 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-modem",
   "metadata": {},
   "source": [
    "Note that before running each section, the kernel must be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-possibility",
   "metadata": {},
   "source": [
    "Note that the current path should be ``Deep-learning-enhanced-Rydberg-multifrequency-microwave-recognition/notebook``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-athletics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:32.558039Z",
     "start_time": "2021-10-14T03:08:32.539053Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../data/4_bins/4bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-princeton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:35.211973Z",
     "start_time": "2021-10-14T03:08:33.985479Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-stock",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:35.227931Z",
     "start_time": "2021-10-14T03:08:35.213955Z"
    }
   },
   "outputs": [],
   "source": [
    "max_min = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "cwd = os.getcwd()\n",
    "# Read data and scale it by its maximum and minimum.\n",
    "def prepare_data():    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)],[str(0)])):        \n",
    "        path = os.path.join(cwd, ''.join(i))        \n",
    "        names = os.listdir(path)       \n",
    "        \n",
    "        for name in names:            \n",
    "            X.append(pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[2]).apply(max_min).to_numpy())            \n",
    "            Y.append(np.array(eval('[' + ','.join(i) + ']')))            \n",
    "        \n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-uruguay",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:36.788941Z",
     "start_time": "2021-10-14T03:08:35.794778Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-coffee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:36.866420Z",
     "start_time": "2021-10-14T03:08:36.849466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into test set and the rest set, the size of test set is 20% of total data set\n",
    "# The test set and the rest set are shuffled randomly.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-algeria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:37.540785Z",
     "start_time": "2021-10-14T03:08:37.507856Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_d = np.zeros((X_train.shape[0]*2, X_train.shape[1],  X_train.shape[2]))\n",
    "y_train_d = np.zeros((y_train.shape[0]*2, y_train.shape[1]))\n",
    "# Here the white noise is added to increase the robustness of the model and increase the size of the training set, \n",
    "# which is quantifyed by its standard deviation \"scale\".\n",
    "# If scale = 0, there is no white noise.\n",
    "# After the additional noise is added, the training set is re-scaled by its maximum and minimum.\n",
    "\n",
    "for id  in range(X_train.shape[0]):\n",
    "    X_train_d[id] = X_train[id]\n",
    "    y_train_d[id] = y_train[id]\n",
    "    X_train_d[id + X_train.shape[0]] = max_min(X_train[id] + np.random.normal(loc=0, scale=0.1, size = (X_train.shape[1], X_train.shape[2])))\n",
    "    y_train_d[id + X_train.shape[0]] = y_train[id]\n",
    "\n",
    "X_train = X_train_d\n",
    "y_train = y_train_d\n",
    "del X_train_d\n",
    "del y_train_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-enterprise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:38.518058Z",
     "start_time": "2021-10-14T03:08:38.508060Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-patrick",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:41.018884Z",
     "start_time": "2021-10-14T03:08:39.105598Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-stadium",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:41.034747Z",
     "start_time": "2021-10-14T03:08:41.020731Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-advancement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:41.891718Z",
     "start_time": "2021-10-14T03:08:41.873727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deep learning model layers\n",
    "def bulid_model_CNN():  \n",
    "    \n",
    "    model_list = []   \n",
    "    \n",
    "    cnn_out_1 = 20 #16\n",
    "    cnn_len_1 = 460 #20\n",
    "    \n",
    "    model_list.append(\n",
    "            layers.Conv1D(cnn_out_1, cnn_len_1, input_shape=(X_train.shape[1],1)),\n",
    "            ) \n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    model_list.append(layers.Activation('relu'))\n",
    "    model_list.append(layers.MaxPooling1D(3))    \n",
    "   \n",
    "    rnn_out = 16  \n",
    "    lr_rate = 1e-3\n",
    "    \n",
    "    \n",
    "    model_list.append(layers.Bidirectional(layers.LSTM(units=rnn_out, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    \n",
    "    model_list.append(layers.Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    model = models.Sequential(model_list)   \n",
    "    \n",
    "    \n",
    "    opt = RMSprop(lr=lr_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-heater",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:42.580124Z",
     "start_time": "2021-10-14T03:08:42.570101Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-rotation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:43.187537Z",
     "start_time": "2021-10-14T03:08:43.169585Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-cruise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:08:46.336613Z",
     "start_time": "2021-10-14T03:08:46.330629Z"
    }
   },
   "outputs": [],
   "source": [
    "# These callback function to schedule the learning rate and save weights of the model during training.\n",
    "filepath = \"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "callbacks_list = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10), \n",
    "                   ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='min')  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-battlefield",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:19:26.817170Z",
     "start_time": "2021-10-14T03:08:49.386284Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 4  # k-fold cross validation\n",
    "num_val_samples = X_train.shape[0] //k\n",
    "num_epochs = 75\n",
    "all_scores = []\n",
    "all_scores_history = []\n",
    "train_loss_history_list = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    K.clear_session()  # clear previous wieghts to avoid cross-talk\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [X_train[:i * num_val_samples],\n",
    "         X_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)              \n",
    "    model = bulid_model_CNN()\n",
    "\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=64, verbose=0, validation_data=(val_data, val_targets),\n",
    "                        callbacks=callbacks_list)\n",
    "        \n",
    "    # Evaluate the model on the validation data\n",
    "    mae_history = history.history['val_loss']\n",
    "    train_loss_history = history.history['loss']    \n",
    "\n",
    "    all_scores_history.append(mae_history)\n",
    "    train_loss_history_list.append(train_loss_history)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-cornell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:20:55.870892Z",
     "start_time": "2021-10-14T03:20:55.707152Z"
    }
   },
   "outputs": [],
   "source": [
    "averaged_history_val_loss = np.mean(np.array(all_scores_history[1:]), axis=0)\n",
    "averaged_history_train_loss = np.mean(np.array(train_loss_history_list[1:]), axis=0)\n",
    "font_size = 16\n",
    "# plot loss on training set and validation set \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(averaged_history_train_loss)\n",
    "plt.plot(averaged_history_val_loss)\n",
    "plt.title('Model loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.ylabel('Loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.xlabel('Epoch', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.legend(['Train', 'Validation'], loc='upper right', prop={'family':'Times New Roman', 'size':font_size})\n",
    "plt.xticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "#plt.savefig('loss.eps',dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-strand",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:00.234693Z",
     "start_time": "2021-10-14T03:21:00.218682Z"
    }
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-silly",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:00.996196Z",
     "start_time": "2021-10-14T03:21:00.737869Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-negotiation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:02.117311Z",
     "start_time": "2021-10-14T03:21:02.105306Z"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(pres, y_test):\n",
    "    cm = np.zeros((8,8))\n",
    "    for id, pre in enumerate(pres):\n",
    "        column = int(pre[0]*4 + pre[1] * 2 + pre[2] * 1 )\n",
    "        row = int(y_test[id, 0]*4 + y_test[id, 1] * 2 + y_test[id, 2] * 1)\n",
    "        cm[row, column] += 1\n",
    "    return cm\n",
    "\n",
    "def heatmap(ma, filename):\n",
    "    labels = [\"000\", \"001\",\"010\",\"011\",\"100\",\"101\",\"110\",\"111\"]\n",
    "    acc = np.round(np.sum(np.diag(ma))/np.sum(ma), 5)\n",
    "    ma = pd.DataFrame(ma, index=labels, columns=labels)\n",
    "    f,ax = plt.subplots(figsize=(9, 6))\n",
    "    ax = sns.heatmap(ma, annot=True, center=11, cmap='RdYlBu')\n",
    "    plt.xlabel(\"Predicted labels\",fontdict={'size':16})\n",
    "    plt.ylabel(\"Target labels\",fontdict={'size':16})\n",
    "    plt.title('Accuracy = %s %%'%str(acc*100))\n",
    "    #plt.savefig('%s.eps'%str(filename),dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-communications",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:03.243587Z",
     "start_time": "2021-10-14T03:21:03.234598Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-wrist",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:13.563214Z",
     "start_time": "2021-10-14T03:21:04.114674Z"
    }
   },
   "outputs": [],
   "source": [
    "# The model_names may be different, please check the data/4_bins/weights/ folder\n",
    "models_names = ['weights-improvement-01-0.17.hdf5', 'weights-improvement-17-0.02.hdf5', 'weights-improvement-30-0.01.hdf5']\n",
    "for id, name in  enumerate(models_names):\n",
    "    model = load_model(name)\n",
    "    pre = model.predict(X_test)\n",
    "    cm = confusion_matrix(np.around(pre), y_test)\n",
    "    heatmap(cm, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-belle",
   "metadata": {},
   "source": [
    "# 20 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-mediterranean",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-filename",
   "metadata": {},
   "source": [
    "Note that before running each section, the kernel must be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-melbourne",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:23.744701Z",
     "start_time": "2021-10-14T03:21:23.731686Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../data/20_bins/20bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-obligation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:26.213558Z",
     "start_time": "2021-10-14T03:21:25.016638Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-dublin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:26.229474Z",
     "start_time": "2021-10-14T03:21:26.214513Z"
    }
   },
   "outputs": [],
   "source": [
    "max_min = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "cwd = os.getcwd()\n",
    "def prepare_data():    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)])):        \n",
    "        path = os.path.join(cwd, ''.join(i))        \n",
    "        names = os.listdir(path)       \n",
    "        \n",
    "        for name in names:            \n",
    "            X.append(pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[2]).apply(max_min).to_numpy())            \n",
    "            Y.append(np.array(eval('[' + ','.join(i) + ']')))            \n",
    "        \n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-mouse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:31.336596Z",
     "start_time": "2021-10-14T03:21:27.681607Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-nursery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:31.352596Z",
     "start_time": "2021-10-14T03:21:31.337614Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-remove",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:31.384486Z",
     "start_time": "2021-10-14T03:21:31.354554Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_d = np.zeros((X_train.shape[0]*2, X_train.shape[1],  X_train.shape[2]))\n",
    "y_train_d = np.zeros((y_train.shape[0]*2, y_train.shape[1]))\n",
    "\n",
    "for id  in range(X_train.shape[0]):\n",
    "    X_train_d[id] = X_train[id]\n",
    "    y_train_d[id] = y_train[id]\n",
    "    X_train_d[id + X_train.shape[0]] = max_min(X_train[id] + np.random.normal(loc=0, scale=0.1, size = (X_train.shape[1], X_train.shape[2])))\n",
    "    y_train_d[id + X_train.shape[0]] = y_train[id]\n",
    "\n",
    "X_train = X_train_d\n",
    "y_train = y_train_d\n",
    "del X_train_d\n",
    "del y_train_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-probability",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:31.400467Z",
     "start_time": "2021-10-14T03:21:31.386504Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-tobago",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:33.425118Z",
     "start_time": "2021-10-14T03:21:31.401428Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-nashville",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:33.440903Z",
     "start_time": "2021-10-14T03:21:33.426941Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-psychiatry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:33.531754Z",
     "start_time": "2021-10-14T03:21:33.511808Z"
    }
   },
   "outputs": [],
   "source": [
    "def bulid_model_CNN():  \n",
    "    \n",
    "    model_list = []   \n",
    "    \n",
    "    cnn_out_1 = 20 #16\n",
    "    cnn_len_1 = 460 #20\n",
    "    \n",
    "    model_list.append(\n",
    "            layers.Conv1D(cnn_out_1, cnn_len_1, input_shape=(X_train.shape[1],1)),\n",
    "            ) \n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    model_list.append(layers.Activation('relu'))\n",
    "    model_list.append(layers.MaxPooling1D(3))      \n",
    "    \n",
    "    rnn_out = 16  \n",
    "    lr_rate = 1e-3\n",
    "    \n",
    "    \n",
    "    model_list.append(layers.Bidirectional(layers.LSTM(units=rnn_out, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    \n",
    "    model_list.append(layers.Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    model = models.Sequential(model_list)   \n",
    "    \n",
    "    \n",
    "    opt = RMSprop(lr=lr_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-superintendent",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:34.574736Z",
     "start_time": "2021-10-14T03:21:34.560774Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-biology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:35.336120Z",
     "start_time": "2021-10-14T03:21:35.305165Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-easter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:21:37.035807Z",
     "start_time": "2021-10-14T03:21:37.025826Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "callbacks_list = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10),\n",
    "                   ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='min')  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-software",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:32:22.397231Z",
     "start_time": "2021-10-14T03:21:38.433497Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_samples = X_train.shape[0] //k\n",
    "num_epochs = 75\n",
    "all_scores = []\n",
    "all_scores_history = []\n",
    "train_loss_history_list = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    K.clear_session()  # memory clean \n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [X_train[:i * num_val_samples],\n",
    "         X_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)              \n",
    "    model = bulid_model_CNN()\n",
    "\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=64, verbose=0, validation_data=(val_data, val_targets),\n",
    "                        callbacks=callbacks_list)\n",
    "        \n",
    "    # Evaluate the model on the validation data\n",
    "    mae_history = history.history['val_loss']\n",
    "    train_loss_history = history.history['loss']    \n",
    "\n",
    "    all_scores_history.append(mae_history)\n",
    "    train_loss_history_list.append(train_loss_history)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-capability",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:36:20.111272Z",
     "start_time": "2021-10-14T03:36:19.751235Z"
    }
   },
   "outputs": [],
   "source": [
    "averaged_history_val_loss = np.mean(np.array(all_scores_history[1:]), axis=0)\n",
    "averaged_history_train_loss = np.mean(np.array(train_loss_history_list[1:]), axis=0)\n",
    "font_size = 16\n",
    "\n",
    "# plot loss on training set and validation set \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(averaged_history_train_loss)\n",
    "plt.plot(averaged_history_val_loss)\n",
    "plt.title('Model loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.ylabel('Loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.xlabel('Epoch', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.legend(['Train', 'Test'], loc='upper right', prop={'family':'Times New Roman', 'size':font_size})\n",
    "plt.xticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "#plt.savefig('loss.eps',dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-restaurant",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:36:29.071085Z",
     "start_time": "2021-10-14T03:36:28.960362Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-therapy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:36:29.700443Z",
     "start_time": "2021-10-14T03:36:29.689473Z"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(pres, y_test):\n",
    "    cm = np.zeros((8,8))\n",
    "    for id, pre in enumerate(pres):\n",
    "        column = int(pre[0]*4 + pre[1] * 2 + pre[2] * 1 )\n",
    "        row = int(y_test[id, 0]*4 + y_test[id, 1] * 2 + y_test[id, 2] * 1)\n",
    "        cm[row, column] += 1\n",
    "    return cm\n",
    "\n",
    "def heatmap(ma, filename):\n",
    "    labels = [\"000\", \"001\",\"010\",\"011\",\"100\",\"101\",\"110\",\"111\"]\n",
    "    acc = np.round(np.sum(np.diag(ma))/np.sum(ma), 5)\n",
    "    ma = pd.DataFrame(ma, index=labels, columns=labels)\n",
    "    f,ax = plt.subplots(figsize=(9, 6))\n",
    "    ax = sns.heatmap(ma, annot=True, center=11, cmap='RdYlBu')\n",
    "    plt.xlabel(\"Predicted labels\",fontdict={'size':16})\n",
    "    plt.ylabel(\"Target labels\",fontdict={'size':16})\n",
    "    plt.title('Accuracy = %s %%'%str(acc*100))\n",
    "    #plt.savefig('%s.eps'%str(filename),dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-mystery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:36:30.562101Z",
     "start_time": "2021-10-14T03:36:30.552128Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-retailer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:36:31.373931Z",
     "start_time": "2021-10-14T03:36:31.360966Z"
    }
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-steering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:36:42.055869Z",
     "start_time": "2021-10-14T03:36:32.120471Z"
    }
   },
   "outputs": [],
   "source": [
    "models_names = ['weights-improvement-29-0.04.hdf5','weights-improvement-58-0.03.hdf5','weights-improvement-64-0.02.hdf5']\n",
    "for id, name in  enumerate(models_names):\n",
    "    model = load_model(name)\n",
    "    pre = model.predict(X_test)\n",
    "    cm = confusion_matrix(np.around(pre), y_test)\n",
    "    heatmap(cm, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-modification",
   "metadata": {},
   "source": [
    "## Master equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-california",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:36:46.680496Z",
     "start_time": "2021-10-14T03:36:46.665531Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-privilege",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:36:49.537027Z",
     "start_time": "2021-10-14T03:36:49.529040Z"
    }
   },
   "outputs": [],
   "source": [
    "def heatmap(ma, filename):\n",
    "    labels = [\"000\", \"001\",\"010\",\"011\",\"100\",\"101\",\"110\",\"111\"]\n",
    "    acc = np.round(np.sum(np.diag(ma))/np.sum(ma), 5)\n",
    "    ma = pd.DataFrame(ma, index=labels, columns=labels)\n",
    "    f,ax = plt.subplots(figsize=(9, 6))\n",
    "    ax = sns.heatmap(ma, annot=True, center=11, cmap='RdYlBu')\n",
    "    plt.xlabel(\"Predicted labels\",fontdict={'size':16})\n",
    "    plt.ylabel(\"Target labels\",fontdict={'size':16})\n",
    "    plt.title('Accuracy = %s %%'%str(acc*100))\n",
    "    #plt.savefig('%s.eps'%str(filename),dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-burton",
   "metadata": {},
   "source": [
    "The data below come from the results of Mathematica notebook \"/notebook/Fig5/20bins_fitting.nb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-recording",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:36:51.677266Z",
     "start_time": "2021-10-14T03:36:51.065914Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_master=np.concatenate([np.histogram([0,0,0,0,0,0,0,6,0,0,0,0,0,0,7,0,0,0,0,0],bins=np.arange(9))[0],\n",
    "np.histogram([0,0,0,1,4,4,4,0,0,0,0,1,4,0,0,0,0,0,0,7],bins=np.arange(9))[0],\n",
    "np.histogram([0,0,6,2,2,0,2,0,2,0,2,1,0,2,2,2,0,2,0,2],bins=np.arange(9))[0],\n",
    "np.histogram([0,0,0,0,1,0,0,5,2,0,0,0,0,0,0,0,0,0,0,0],bins=np.arange(9))[0],\n",
    "np.histogram([1,6,6,0,6,1,1,1,0,1,1,6,1,0,0,0,6,0,5,0],bins=np.arange(9))[0],\n",
    "np.histogram([0,0,0,0,5,5,2,0,0,0,0,0,2,0,0,2,2,2,0,2],bins=np.arange(9))[0],\n",
    "np.histogram([3,0,3,3,0,3,3,4,1,4,3,0,2,3,4,0,0,0,3,3],bins=np.arange(9))[0],\n",
    "np.histogram([0,0,0,0,7,0,0,0,3,0,5,0,0,0,0,0,0,0,1,0],bins=np.arange(9))[0]],axis=0).reshape(8,8)\n",
    "heatmap(cm_master,'20bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-inflation",
   "metadata": {},
   "source": [
    "# 200 Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-screw",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-garden",
   "metadata": {},
   "source": [
    "Note that before running each section, the kernel must be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-dayton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:08.988833Z",
     "start_time": "2021-10-14T03:37:08.975813Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../data/resolution/resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-terrorist",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:11.361470Z",
     "start_time": "2021-10-14T03:37:10.032984Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-jason",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:11.377389Z",
     "start_time": "2021-10-14T03:37:11.363427Z"
    }
   },
   "outputs": [],
   "source": [
    "max_min = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "cwd = os.getcwd()\n",
    "\n",
    "def prepare_data():    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)])):        \n",
    "        path = os.path.join(cwd, ''.join(i))        \n",
    "        names = os.listdir(path)       \n",
    "        \n",
    "        for name in names:            \n",
    "            X.append(pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[2]).apply(max_min).to_numpy())            \n",
    "            Y.append(np.array(eval('[' + ','.join(i) + ']')))            \n",
    "        \n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-juice",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:16.201858Z",
     "start_time": "2021-10-14T03:37:12.522663Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-circular",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:16.217779Z",
     "start_time": "2021-10-14T03:37:16.203822Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-thinking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:16.265650Z",
     "start_time": "2021-10-14T03:37:16.219773Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_d = np.zeros((X_train.shape[0]*2, X_train.shape[1],  X_train.shape[2]))\n",
    "y_train_d = np.zeros((y_train.shape[0]*2, y_train.shape[1]))\n",
    "\n",
    "for id  in range(X_train.shape[0]):\n",
    "    X_train_d[id] = X_train[id]\n",
    "    y_train_d[id] = y_train[id]\n",
    "    X_train_d[id + X_train.shape[0]] = max_min(X_train[id] + np.random.normal(loc=0, scale=0.5, size = (X_train.shape[1], X_train.shape[2])))\n",
    "    y_train_d[id + X_train.shape[0]] = y_train[id]\n",
    "\n",
    "X_train = X_train_d\n",
    "y_train = y_train_d\n",
    "del X_train_d\n",
    "del y_train_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-feeding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:16.281608Z",
     "start_time": "2021-10-14T03:37:16.267645Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-guitar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:18.405977Z",
     "start_time": "2021-10-14T03:37:16.283602Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-satellite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:18.421883Z",
     "start_time": "2021-10-14T03:37:18.408918Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-bachelor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:19.311541Z",
     "start_time": "2021-10-14T03:37:19.296564Z"
    }
   },
   "outputs": [],
   "source": [
    "def bulid_model_CNN():  \n",
    "    \n",
    "    model_list = []   \n",
    "    \n",
    "    cnn_out_1 = 20 #16\n",
    "    cnn_len_1 = 460 #20\n",
    "    \n",
    "    model_list.append(\n",
    "            layers.Conv1D(cnn_out_1, cnn_len_1, input_shape=(X_train.shape[1],1)),\n",
    "            ) \n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    model_list.append(layers.Activation('relu'))\n",
    "    model_list.append(layers.MaxPooling1D(3))  \n",
    "        \n",
    "    rnn_out = 16  \n",
    "    lr_rate = 1e-3\n",
    "    \n",
    "    \n",
    "    model_list.append(layers.Bidirectional(layers.LSTM(units=rnn_out, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    \n",
    "    model_list.append(layers.Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    model = models.Sequential(model_list)   \n",
    "    \n",
    "    \n",
    "    opt = RMSprop(lr=lr_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-tobago",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:20.474393Z",
     "start_time": "2021-10-14T03:37:20.465418Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-yellow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:21.007007Z",
     "start_time": "2021-10-14T03:37:20.999988Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-lawrence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:37:22.116016Z",
     "start_time": "2021-10-14T03:37:22.098050Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "callbacks_list = [ ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10), \n",
    "                  ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='min') ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-velvet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T03:51:06.500389Z",
     "start_time": "2021-10-14T03:37:23.928155Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_samples = X_train.shape[0] //k\n",
    "num_epochs = 75\n",
    "all_scores = []\n",
    "all_scores_history = []\n",
    "train_loss_history_list = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    K.clear_session()  # memory clean\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [X_train[:i * num_val_samples],\n",
    "         X_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)              \n",
    "    model = bulid_model_CNN()\n",
    "\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=64, verbose=0, validation_data=(val_data, val_targets), callbacks=callbacks_list)\n",
    "        \n",
    "    # Evaluate the model on the validation data\n",
    "    mae_history = history.history['val_loss']\n",
    "    train_loss_history = history.history['loss']    \n",
    "\n",
    "    all_scores_history.append(mae_history)\n",
    "    train_loss_history_list.append(train_loss_history)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-hanging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:36:49.279861Z",
     "start_time": "2021-10-14T04:36:48.934334Z"
    }
   },
   "outputs": [],
   "source": [
    "averaged_history_val_loss = np.mean(np.array(all_scores_history[1:]), axis=0)\n",
    "averaged_history_train_loss = np.mean(np.array(train_loss_history_list[1:]), axis=0)\n",
    "font_size = 16\n",
    "\n",
    "# plot loss on training set and validation set \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(averaged_history_train_loss)\n",
    "plt.plot(averaged_history_val_loss)\n",
    "plt.title('Model loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.ylabel('Loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.xlabel('Epoch', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.legend(['Train', 'Test'], loc='upper right', prop={'family':'Times New Roman', 'size':font_size})\n",
    "plt.xticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "#plt.savefig('loss.eps',dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-watch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:36:52.964713Z",
     "start_time": "2021-10-14T04:36:52.798875Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-absolute",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:36:53.930084Z",
     "start_time": "2021-10-14T04:36:53.911082Z"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(pres, y_test):\n",
    "    cm = np.zeros((8,8))\n",
    "    for id, pre in enumerate(pres):\n",
    "        column = int(pre[0]*4 + pre[1] * 2 + pre[2] * 1 )\n",
    "        row = int(y_test[id, 0]*4 + y_test[id, 1] * 2 + y_test[id, 2] * 1)\n",
    "        cm[row, column] += 1\n",
    "    return cm\n",
    "\n",
    "def heatmap(ma, filename):\n",
    "    labels = [\"000\", \"001\",\"010\",\"011\",\"100\",\"101\",\"110\",\"111\"]\n",
    "    acc = np.round(np.sum(np.diag(ma))/np.sum(ma), 5)\n",
    "    ma = pd.DataFrame(ma, index=labels, columns=labels)\n",
    "    f,ax = plt.subplots(figsize=(9, 6))\n",
    "    ax = sns.heatmap(ma, annot=True, center=11, cmap='RdYlBu')\n",
    "    plt.xlabel(\"Predicted labels\",fontdict={'size':16})\n",
    "    plt.ylabel(\"Target labels\",fontdict={'size':16})\n",
    "    plt.title('Accuracy = %s %%'%str(acc*100))\n",
    "    #plt.savefig('%s.eps'%str(filename),dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-intent",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:36:54.429875Z",
     "start_time": "2021-10-14T04:36:54.412921Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-rainbow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:37:04.306230Z",
     "start_time": "2021-10-14T04:36:54.869434Z"
    }
   },
   "outputs": [],
   "source": [
    "models_names = ['weights-improvement-13-0.11.hdf5', 'weights-improvement-57-0.02.hdf5', 'weights-improvement-100-0.03.hdf5']\n",
    "for id, name in  enumerate(models_names):\n",
    "    model = load_model(name)\n",
    "    pre = model.predict(X_test)\n",
    "    cm = confusion_matrix(np.around(pre), y_test)\n",
    "    heatmap(cm, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-alaska",
   "metadata": {},
   "source": [
    "## Mster equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-incidence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:37:55.264778Z",
     "start_time": "2021-10-14T04:37:55.255835Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-shell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:37:56.015543Z",
     "start_time": "2021-10-14T04:37:55.798157Z"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(pres, y_test):\n",
    "    cm = np.zeros((8,8))\n",
    "    for id, pre in enumerate(pres):\n",
    "        column = int(pre[0]*4 + pre[1] * 2 + pre[2] * 1 )\n",
    "        row = int(y_test[id, 0]*4 + y_test[id, 1] * 2 + y_test[id, 2] * 1)\n",
    "        cm[row, column] += 1\n",
    "    return cm\n",
    "\n",
    "def heatmap(ma, filename):\n",
    "    labels = [\"000\", \"001\",\"010\",\"011\",\"100\",\"101\",\"110\",\"111\"]\n",
    "    acc = np.round(np.sum(np.diag(ma))/np.sum(ma), 5)\n",
    "    ma = pd.DataFrame(ma, index=labels, columns=labels)\n",
    "    f,ax = plt.subplots(figsize=(9, 6))\n",
    "    ax = sns.heatmap(ma, annot=True, center=11, cmap='RdYlBu')\n",
    "    plt.xlabel(\"Predicted labels\",fontdict={'size':16})\n",
    "    plt.ylabel(\"Target labels\",fontdict={'size':16})\n",
    "    plt.title('Accuracy = %s %%'%str(acc*100))\n",
    "    #plt.savefig('%s.eps'%str(filename),dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-appearance",
   "metadata": {},
   "source": [
    "The data below come from the results of Mathematica notebook \"/notebook/Fig5/200MHz_fitting.nb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-playing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:37:58.491366Z",
     "start_time": "2021-10-14T04:37:58.092829Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_master=np.concatenate([np.histogram([0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0],bins=np.arange(9))[0],\n",
    "np.histogram([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 5, 1, 1, 1, 1],bins=np.arange(9))[0],\n",
    "np.histogram([6, 2, 2, 2, 2, 2, 1, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 2],bins=np.arange(9))[0],\n",
    "np.histogram([7, 7, 3, 7, 7, 0, 3, 7, 0, 3, 3, 7, 3, 3, 3, 7, 3, 3, 3, 3],bins=np.arange(9))[0],\n",
    "np.histogram([0, 4, 0, 0, 4, 4, 4, 4, 0, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 4],bins=np.arange(9))[0],\n",
    "np.histogram([0, 5, 5, 1, 1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 0, 0, 3, 1, 0, 5],bins=np.arange(9))[0],\n",
    "np.histogram([6, 6, 0, 0, 6, 6, 0, 6, 0, 6, 4, 6, 2, 6, 6, 6, 0, 6, 6, 7],bins=np.arange(9))[0],\n",
    "np.histogram([3, 7, 0, 7, 7, 0, 6, 7, 7, 3, 0, 0, 0, 0, 7, 2, 7, 1, 7, 0],bins=np.arange(9))[0]],axis=0).reshape(8,8)\n",
    "heatmap(cm_master,'200Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-kingston",
   "metadata": {},
   "source": [
    "# Data for additional noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-field",
   "metadata": {},
   "source": [
    "Note that before running each section, the kernel must be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-reply",
   "metadata": {},
   "source": [
    "The code in this section is to prepare the data with white noise for the master equation fitting in `4bins_fitting.nb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-official",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T12:51:40.382207Z",
     "start_time": "2021-10-13T12:51:40.369760Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ..\\data\\additional_noise\\additional_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-village",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T12:51:43.824192Z",
     "start_time": "2021-10-13T12:51:43.249166Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "scale_list = np.arange(0.05, 5.5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-harvest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T12:52:38.241503Z",
     "start_time": "2021-10-13T12:51:46.870402Z"
    }
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "cwd = os.getcwd()\n",
    "max_min = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "\n",
    "for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)],str(0))):        \n",
    "    path = os.path.join(cwd, ''.join(i))        \n",
    "    names = os.listdir(path)       \n",
    "\n",
    "    for name in names: \n",
    "        t = pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[0]).to_numpy()\n",
    "        X = (pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[2]).apply(max_min).to_numpy())            \n",
    "        Y.append(np.array(eval('[' + ','.join(i) + ']')))   \n",
    "        \n",
    "        for scale in scale_list:\n",
    "            X_new = np.zeros_like(X)\n",
    "            X_new = (X + np.random.normal(loc=0, scale=scale, size = X.shape))\n",
    "                        \n",
    "            try:\n",
    "                os.mkdir(''.join(i)+'/'+\"{0:.2f}\".format(scale))\n",
    "\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "            \n",
    "            np.savetxt(''.join(i)+'/'+\"{0:.2f}\".format(scale)+'/'+str(name)[:-4]+'.txt', np.c_[t ,max_min(X_new)] ,delimiter='\\t', fmt='%.4e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-participant",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T06:51:15.334000Z",
     "start_time": "2021-10-13T06:51:11.746175Z"
    }
   },
   "outputs": [],
   "source": [
    "# codes to delete the signals with white noise, usually donot need to run.\n",
    "\"\"\"\n",
    "import shutil\n",
    "\n",
    "cwd = os.getcwd()\n",
    "for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)],str(0))):   \n",
    "    path = os.path.join(cwd, ''.join(i))  \n",
    "    for scale in scale_list:\n",
    "        path1 = os.path.join(path, \"{0:.2f}\".format(scale))\n",
    "        try:\n",
    "            shutil.rmtree(path1)\n",
    "        except (FileNotFoundError, PermissionError) as e:\n",
    "            print(\"Error: %s : %s\" % (path1, e.strerror))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-omaha",
   "metadata": {},
   "source": [
    "# Additional Noises for 4 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-israel",
   "metadata": {},
   "source": [
    "Note that before running each section, the kernel must be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-craft",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:38:13.348006Z",
     "start_time": "2021-10-14T04:38:13.334992Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../data/4_bins/4bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-metabolism",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:38:18.788372Z",
     "start_time": "2021-10-14T04:38:13.851968Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 24\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks_list = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)] \n",
    "from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-cologne",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T04:38:18.803642Z",
     "start_time": "2021-10-14T04:38:18.790624Z"
    }
   },
   "outputs": [],
   "source": [
    "train_scale_list = np.arange(0.0, 3.5, 0.1) # The standard deviation of white noise in the training set\n",
    "model_list = []\n",
    "max_min = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "cwd = os.getcwd()\n",
    "\n",
    "def prepare_data():    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)],[str(0)])):        \n",
    "        path = os.path.join(cwd, ''.join(i))        \n",
    "        names = os.listdir(path)       \n",
    "        \n",
    "        for name in names:            \n",
    "            X.append(pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[2]).apply(max_min).to_numpy())            \n",
    "            Y.append(np.array(eval('[' + ','.join(i) + ']')))            \n",
    "        \n",
    "    return np.array(X), np.array(Y)[:,:3]\n",
    "\n",
    "def bulid_model_CNN():  \n",
    "    \n",
    "    model_list = []   \n",
    "    \n",
    "    cnn_out_1 = 20 #16\n",
    "    cnn_len_1 = 460 #20\n",
    "    \n",
    "    model_list.append(\n",
    "            layers.Conv1D(cnn_out_1, cnn_len_1, input_shape=(X_train.shape[1],1)),\n",
    "            ) \n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    model_list.append(layers.Activation('relu'))\n",
    "    model_list.append(layers.MaxPooling1D(3))     \n",
    "    \n",
    "    rnn_out = 16  \n",
    "    lr_rate = 1e-3\n",
    "    \n",
    "    \n",
    "    model_list.append(layers.Bidirectional(layers.LSTM(units=rnn_out, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    \n",
    "    model_list.append(layers.Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    model = models.Sequential(model_list)   \n",
    "    \n",
    "    \n",
    "    opt = RMSprop(lr=lr_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-running",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T11:25:27.641918Z",
     "start_time": "2021-10-14T04:38:18.806583Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for scale in train_scale_list:\n",
    "    \n",
    "    X, Y = prepare_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=7)\n",
    "\n",
    "    X_train_d = np.zeros((X_train.shape[0]*2, X_train.shape[1],  X_train.shape[2]))\n",
    "    y_train_d = np.zeros((y_train.shape[0]*2, y_train.shape[1]))\n",
    "\n",
    "    for id  in range(X_train.shape[0]):\n",
    "        X_train_d[id] = X_train[id]\n",
    "        y_train_d[id] = y_train[id]\n",
    "        X_train_d[id + X_train.shape[0]] =(X_train[id] + np.random.normal(loc=0, scale=scale, size = (X_train.shape[1], X_train.shape[2])))\n",
    "        y_train_d[id + X_train.shape[0]] = y_train[id]\n",
    "\n",
    "    X_train = X_train_d\n",
    "    y_train = y_train_d\n",
    "    del X_train_d\n",
    "    del y_train_d\n",
    "\n",
    "    k = 4\n",
    "    num_val_samples = X_train.shape[0] //k\n",
    "    num_epochs = 75\n",
    "\n",
    "    for i in range(k):\n",
    "        print('processing fold #', i)\n",
    "        K.clear_session() \n",
    "        # Prepare the validation data: data from partition # k\n",
    "        val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        # Prepare the training data: data from all other partitions\n",
    "        partial_train_data = np.concatenate(\n",
    "            [X_train[:i * num_val_samples],\n",
    "             X_train[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [y_train[:i * num_val_samples],\n",
    "             y_train[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "                   \n",
    "        model = bulid_model_CNN()\n",
    "\n",
    "        history = model.fit(partial_train_data, partial_train_targets,\n",
    "                  epochs=num_epochs, batch_size=64, verbose=0, validation_data=(val_data, val_targets),\n",
    "                            callbacks=callbacks_list)\n",
    "\n",
    "    model.save(str(scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-latitude",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T11:42:51.210032Z",
     "start_time": "2021-10-14T11:25:27.643920Z"
    }
   },
   "outputs": [],
   "source": [
    "scale_list=np.arange(0.0, 5.5, 0.1) # The standard deviation of white noise in the test set\n",
    "acc_mat = []\n",
    "for scale_train in train_scale_list:\n",
    "    acc_list = np.zeros((5, len(scale_list)))\n",
    "    reconstructed_model = models.load_model(str(scale_train))\n",
    "    for ii in range(5):\n",
    "\n",
    "        for ind, scale in enumerate( scale_list):\n",
    "            X_test_d = np.zeros((X_test.shape[0], X_test.shape[1],  X_test.shape[2]))\n",
    "            y_test_d = np.zeros((y_test.shape[0], y_test.shape[1]))\n",
    "\n",
    "            for id  in range(X_test.shape[0]):\n",
    "                X_test_d[id] = (X_test[id] + np.random.normal(loc=0, scale=scale, size = (X_test.shape[1], X_test.shape[2])))\n",
    "                y_test_d[id] = y_test[id] \n",
    "                \n",
    "            pre = reconstructed_model.predict(X_test_d)            \n",
    "            accuracy = sum([1 if np.sum(np.abs(np.around(pre[i]) -y_test_d[i])) == 0 else 0 for i in range(len(pre)) ])/len(pre)\n",
    "            acc_list[ii,ind] = accuracy\n",
    "\n",
    "    acc_mat.append(np.mean(np.array(acc_list),axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-composer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T12:08:07.980549Z",
     "start_time": "2021-10-14T12:08:07.815950Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(acc_mat[0],'-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-design",
   "metadata": {},
   "source": [
    "Data is from master equation fitting, which is \"calculated by /notebook/4bins_fitting.nb\"\n",
    "\n",
    "The \"acc_mat0\" is \"acc_mat[0]\" calculated above. It can be replaced by \"acc_mat[0]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-training",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T13:20:18.508442Z",
     "start_time": "2021-10-14T13:20:18.087582Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 0, 3, 4, 4, 4, \\\n",
    "4, 4, 3, 5, 5, 5, 0, 6, 6, 6, 0, 0, 0, 1, 4, 0, 4, 0, 0, 0, 0, 0, 1, \\\n",
    "1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 7, 5, 7, \\\n",
    "3, 4, 3, 7, 7, 0, 6, 0, 7, 6, 4, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, \\\n",
    "2, 2, 2, 3, 3, 5, 3, 7, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 7, 7, 0, 6, \\\n",
    "7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 2, 3, 3, 3, \\\n",
    "3, 3, 4, 4, 4, 4, 4, 2, 5, 5, 0, 5, 6, 0, 7, 7, 6, 7, 7, 7, 7, 1, 0, \\\n",
    "0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 0, 0, 3, 3, 4, 4, 4, 4, \\\n",
    "4, 5, 2, 6, 0, 5, 6, 0, 0, 3, 0, 7, 7, 7, 7, 5, 0, 0, 0, 0, 0, 1, 1, \\\n",
    "6, 1, 0, 2, 2, 2, 0, 2, 3, 5, 3, 3, 3, 0, 0, 4, 0, 4, 0, 2, 6, 2, 4, \\\n",
    "0, 0, 0, 7, 6, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, \\\n",
    "0, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 3, 0, 0, 6, 1, 0, 1, 0, 0, 0, 7, \\\n",
    "7, 7, 7, 7, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 2, 4, 0, 0, 3, \\\n",
    "3, 0, 4, 4, 4, 4, 0, 2, 0, 7, 2, 0, 0, 2, 0, 0, 7, 7, 7, 7, 7, 0, 0, \\\n",
    "0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 0, 0, 0, 0, 3, 4, 0, 4, 4, 4, \\\n",
    "2, 2, 1, 3, 2, 6, 0, 1, 0, 6, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 1, 1, 1, \\\n",
    "1, 0, 2, 2, 2, 5, 0, 4, 0, 4, 3, 0, 0, 4, 4, 0, 4, 2, 2, 0, 0, 2, 0, \\\n",
    "0, 0, 1, 0, 7, 7, 5, 7, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, \\\n",
    "0, 3, 3, 0, 0, 0, 4, 4, 0, 4, 4, 2, 0, 0, 0, 2, 0, 0, 6, 0, 0, 7, 7, \\\n",
    "0, 4, 0, 4, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 0, 2, 0, 4, 0, 1, 4, \\\n",
    "6, 0, 0, 4, 4, 7, 0, 2, 0, 2, 6, 0, 7, 4, 6, 0, 0, 7, 7, 7, 0, 0, 0, \\\n",
    "0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 2, 0, 4, 0, 0, 3, 4, 4, 4, 0, 4, 0, \\\n",
    "0, 4, 1, 0, 0, 0, 0, 1, 0, 0, 3, 0, 7, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, \\\n",
    "0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 0, 4, 4, 4, 4, 4, 2, 2, 0, 0, 0, 6, 0, \\\n",
    "0, 0, 0, 1, 1, 0, 7, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 2, 2, 2, 0, 2, \\\n",
    "0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 7, 0, 0, 0, 0, 0, 0, 1, 6, 0, 1, 6, \\\n",
    "0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 4, 0, 3, 0, 0, 0, \\\n",
    "0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 6, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, \\\n",
    "0, 1, 0, 1, 4, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 4, 4, 0, 2, \\\n",
    "0, 0, 0, 0, 0, 0, 0, 6, 7, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 3, 1, 0, \\\n",
    "2, 0, 2, 2, 3, 0, 4, 4, 0, 3, 4, 0, 4, 4, 0, 0, 1, 0, 0, 6, 0, 0, 0, \\\n",
    "3, 1, 5, 5, 2, 2, 4, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 0, 2, 2, 0, \\\n",
    "0, 0, 0, 0, 0, 6, 0, 0, 4, 7, 6, 0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 0, 1, \\\n",
    "0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2, 2, 0, 0, 7, 0, 0, 0, 0, 0, 0, \\\n",
    "0, 0, 4, 0, 2, 0, 0, 0, 6, 0, 1, 0, 6, 0, 1, 6, 2, 0, 0, 4, 0, 4, 0, \\\n",
    "0, 0, 4, 1, 4, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 1, 0, 0, \\\n",
    "0, 0, 0, 1, 0, 0, 6, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, \\\n",
    "0, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 4, 4, 4, 2, 6, 0, 0, 0, 6, 1, 6, 0, \\\n",
    "0, 0, 0, 0, 4, 1, 4, 0, 0, 0, 0, 0, 0, 7, 3, 1, 2, 2, 2, 2, 6, 0, 6, \\\n",
    "0, 4, 4, 0, 0, 4, 0, 4, 2, 0, 0, 0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 4, 0, \\\n",
    "0, 0, 0, 0, 0, 1, 3, 1, 3, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 4, 4, 4, \\\n",
    "0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 4, 0, 0, 1, \\\n",
    "0, 1, 1, 0, 0, 2, 2, 5, 3, 0, 0, 0, 4, 0, 4, 0, 4, 0, 4, 3, 5, 0, 0, \\\n",
    "2, 0, 0, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 4, 1, 1, 0, 2, 1, 0, 2, \\\n",
    "2, 0, 0, 0, 0, 0, 0, 1, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, \\\n",
    "2, 1, 0, 0, 0, 0, 0, 4, 0, 0, 1, 1, 0, 1, 0, 0, 2, 2, 0, 2, 0, 3, 0, \\\n",
    "3, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 1, 6, 0, 0, 0, 6, 2, 0, 5, 0, 0, 0, \\\n",
    "0, 0, 0, 0, 1, 0, 1, 4, 0, 6, 2, 0, 0, 2, 0, 0, 0, 0, 1, 0, 4, 4, 0, \\\n",
    "0, 0, 0, 0, 0, 7, 0, 1, 6, 0, 6, 1, 1, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, \\\n",
    "7, 1, 1, 0, 2, 2, 2, 2, 0, 3, 3, 0, 0, 2, 4, 4, 0, 0, 0, 0, 0, 4, 4, \\\n",
    "6, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 2, 0, 0, \\\n",
    "5, 0, 0, 0, 2, 5, 6, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 3, 0, 0, \\\n",
    "0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 6, 0, 0, 0, 0, 0, 3, 0, \\\n",
    "0, 0, 4, 4, 5, 4, 1, 5, 1, 0, 5, 1, 2, 1, 0, 6, 0, 1, 0, 0, 0, 0, 0, \\\n",
    "0, 0, 0, 0, 1, 1, 3, 1, 2, 2, 0, 0, 4, 5, 0, 3, 0, 3, 0, 0, 4, 4, 4, \\\n",
    "5, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 0, 1, \\\n",
    "0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 5, 0, 0, 0, 0, 6, \\\n",
    "0, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 3, 0, \\\n",
    "0, 0, 0, 0, 0, 3, 0, 2, 2, 4, 4, 0, 5, 1, 0, 0, 0, 0, 4, 5, 0, 6, 0, \\\n",
    "0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 1, 4, 0, 1, 0, 2, 0, 4, 0, 0, 7, 0, \\\n",
    "0, 4, 4, 4, 4, 0, 0, 1, 1, 0, 0, 0, 0, 6, 6, 0, 0, 4, 0, 6, 0, 0, 0, \\\n",
    "0, 0, 0, 1, 1, 0, 1, 6, 0, 3, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, \\\n",
    "0, 0, 5, 0, 2, 0, 0, 0, 6, 0, 5, 0, 5, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, \\\n",
    "0, 0, 0, 4, 0, 0, 0, 3, 7, 0, 1, 0, 2, 4, 4, 0, 0, 1, 0, 0, 0, 4, 0, \\\n",
    "4, 0, 2, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 2, 1, 1, 0, 2, 0, 0, 0, \\\n",
    "0, 0, 7, 5, 0, 0, 0, 4, 0, 6, 1, 1, 4, 0, 0, 1, 0, 0, 1, 0, 6, 7, 0, \\\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 0, 0, 0, 2, 2, 0, 4, 0, 0, 0, 0, 2, \\\n",
    "0, 4, 0, 0, 0, 0, 3, 0, 0, 2, 1, 6, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, \\\n",
    "0, 1, 0, 7, 0, 1, 6, 4, 2, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 4, \\\n",
    "0, 1, 0, 0, 0, 6, 1, 0, 0, 4, 0, 3, 0, 0, 0, 4, 0, 0, 0, 0, 1, 1, 3, \\\n",
    "6, 2, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 4, 1, 0, 0, 5, 0, 6, 0, 0, \\\n",
    "7, 0, 0, 1, 0, 0, 1, 0, 0, 4, 0, 0, 3, 0, 1, 0, 0, 0, 0, 3, 0, 0, 3, \\\n",
    "0, 0, 3, 0, 4, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 1, 0, 6, 4, 0, 0, 0, \\\n",
    "0, 0, 0, 0, 6, 0, 1, 0, 1, 0, 0, 2, 3, 0, 0, 0, 3, 0, 2, 0, 0, 0, 6, \\\n",
    "0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 6, 0, 1, 0, 0, 0, 0, 0, \\\n",
    "0, 1, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 5, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, \\\n",
    "0, 0, 4, 0, 0, 6, 0, 0, 2, 0, 0, 0, 4, 3, 0, 4, 0, 0, 0, 1, 1, 1, 2, \\\n",
    "0, 0, 0, 1, 0, 0, 0, 3, 2, 0, 0, 0, 4, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, \\\n",
    "0, 0, 7, 1, 2, 0, 4, 0, 0, 4, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, \\\n",
    "1, 0, 3, 0, 4, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 5, 4, 4, 0, 0, 0, 4, 0, \\\n",
    "2, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 1, 4, 0, \\\n",
    "0, 2, 5, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 1, 0, 0, 0, 0, 4, 4, 0, 0, 0, \\\n",
    "0, 0, 1, 0, 0, 0, 2, 0, 0, 7, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 3, 0, 3, \\\n",
    "0, 0, 2, 0, 4, 2, 1, 1, 0, 0, 0, 2, 2, 2, 0, 0, 3, 0, 1, 3, 6, 0, 2, \\\n",
    "2, 2, 2, 0, 0, 0, 3, 0, 0, 4, 2, 0, 0, 2, 6, 0, 2, 0, 2, 0, 0, 0, 0, \\\n",
    "0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 5, 4, 0, 1, 0, 2, 0, 0, 0, 3, 0, 4, 0, \\\n",
    "2, 0, 0, 0, 2, 4, 0, 0, 1, 1, 4, 1, 0, 0, 2, 6, 0, 3, 3, 0, 0, 0, 0, \\\n",
    "0, 0, 4, 0, 2, 0, 2, 6, 1, 0, 0, 0, 0, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, \\\n",
    "0, 5, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, \\\n",
    "0, 1, 0, 3, 0, 0, 2, 0, 4, 3, 0, 4, 0, 0, 4, 0, 4, 4, 0, 1, 5, 0, 0, \\\n",
    "2, 0, 6, 0, 6, 1, 0, 2, 4, 0, 0, 0, 0, 0, 2, 0, 1, 3, 1, 0, 2, 0, 0, \\\n",
    "0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 5, 1, 5, 0, 4, 0, 0, 0, 1, 0, \\\n",
    "3, 0, 0, 0, 6, 0, 0, 4, 0, 0, 3, 0, 2, 0, 4, 0, 0, 6, 3, 2, 0, 3, 0, \\\n",
    "0, 0, 4, 0, 3, 7, 1, 0, 4, 4, 0, 0, 0, 2, 0, 2, 0, 0, 5, 4, 0, 0, 4, \\\n",
    "0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 4, 0, 6, 0, \\\n",
    "0, 0, 0, 0, 1, 4, 0, 0, 3, 2, 0, 0, 0, 4, 3])\n",
    "\n",
    "acc_mat0 = np.array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "       1.        , 1.        , 1.        , 0.99473684, 0.99473684,\n",
    "       0.97368421, 0.98421053, 0.96315789, 0.92631579, 0.91052632,\n",
    "       0.9       , 0.85263158, 0.85789474, 0.78947368, 0.77894737,\n",
    "       0.76315789, 0.69473684, 0.68947368, 0.71052632, 0.67894737,\n",
    "       0.71578947, 0.61578947, 0.65263158, 0.59473684, 0.58421053,\n",
    "       0.54210526, 0.51578947, 0.56315789, 0.46842105, 0.52631579,\n",
    "       0.44736842, 0.47894737, 0.46842105, 0.49473684, 0.44210526,\n",
    "       0.38421053, 0.42631579, 0.39473684, 0.41052632, 0.39473684,\n",
    "       0.40526316, 0.41052632, 0.33684211, 0.35263158, 0.44210526,\n",
    "       0.32631579, 0.38947368, 0.31578947, 0.3       , 0.37368421])\n",
    "\n",
    "acc_list = np.zeros(len(data)//40)\n",
    "for j in range(len(data)//40):\n",
    "    acc = []\n",
    "    for i in range(8):\n",
    "        acc.append(len(np.where(data[(40*j+5*i): (40*j+5*i+5)]== i)[0]))\n",
    "    acc_list[j]=sum(acc)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(np.arange(0.05, 5.5, 0.1), acc_list/40,'-o', label='Master equation')\n",
    "plt.plot(np.arange(0, 5.5, 0.1), acc_mat0, '-o', label='Deep learning')\n",
    "plt.xlabel('Test set standard deviation ')\n",
    "plt.ylabel('Test set accuracy')\n",
    "plt.legend()\n",
    "#plt.savefig('testacc.eps',dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-designer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T13:20:22.198650Z",
     "start_time": "2021-10-14T13:20:21.929931Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_mat = np.squeeze(np.array(acc_mat))\n",
    "\n",
    "x = np.arange(0, 5.5, 0.1)\n",
    "y = np.arange(0, 3.5, 0.1)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.rcParams.update({\"font.size\":20})\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.pcolor(X, Y, acc_mat, shading='auto',edgecolors='None',snap=False)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Test set standard deviation ')\n",
    "plt.ylabel('Training set standard deviation')\n",
    "#plt.savefig('noise_map1.eps')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
