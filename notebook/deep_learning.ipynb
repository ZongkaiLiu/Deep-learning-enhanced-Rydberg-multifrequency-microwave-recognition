{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "graphic-founder",
   "metadata": {},
   "source": [
    "Note that before running each section, the kernel must be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-armenia",
   "metadata": {},
   "source": [
    "# 4 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-timeline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:25.507034Z",
     "start_time": "2021-10-13T02:08:25.450136Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../data/4_bins/4bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-management",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:29.915186Z",
     "start_time": "2021-10-13T02:08:26.000006Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-absorption",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:29.931194Z",
     "start_time": "2021-10-13T02:08:29.915186Z"
    }
   },
   "outputs": [],
   "source": [
    "max_min = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "cwd = os.getcwd()\n",
    "# Read data and scale it by its maximum and minimum.\n",
    "def prepare_data():    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)],[str(0)])):        \n",
    "        path = os.path.join(cwd, ''.join(i))        \n",
    "        names = os.listdir(path)       \n",
    "        \n",
    "        for name in names:            \n",
    "            X.append(pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[2]).apply(max_min).to_numpy())            \n",
    "            Y.append(np.array(eval('[' + ','.join(i) + ']')))            \n",
    "        \n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-tours",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:33.071471Z",
     "start_time": "2021-10-13T02:08:29.933152Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-wrestling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:33.086778Z",
     "start_time": "2021-10-13T02:08:33.071471Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into test set and the rest set, the size of test set is 20% of total data set\n",
    "# The test set and the rest set are shuffled randomly.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-province",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:33.117746Z",
     "start_time": "2021-10-13T02:08:33.088784Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_d = np.zeros((X_train.shape[0]*2, X_train.shape[1],  X_train.shape[2]))\n",
    "y_train_d = np.zeros((y_train.shape[0]*2, y_train.shape[1]))\n",
    "# Here the white noise is added to increase the robustness of the model and increase the size of the training set, \n",
    "# which is quantifyed by its standard deviation \"scale\".\n",
    "# If scale = 0, there is no white noise.\n",
    "# After the additional noise is added, the training set is re-scaled by its maximum and minimum.\n",
    "\n",
    "for id  in range(X_train.shape[0]):\n",
    "    X_train_d[id] = X_train[id]\n",
    "    y_train_d[id] = y_train[id]\n",
    "    X_train_d[id + X_train.shape[0]] = max_min(X_train[id] + np.random.normal(loc=0, scale=0.1, size = (X_train.shape[1], X_train.shape[2])))\n",
    "    y_train_d[id + X_train.shape[0]] = y_train[id]\n",
    "\n",
    "X_train = X_train_d\n",
    "y_train = y_train_d\n",
    "del X_train_d\n",
    "del y_train_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-contributor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:33.133703Z",
     "start_time": "2021-10-13T02:08:33.119690Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-supervisor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:35.606399Z",
     "start_time": "2021-10-13T02:08:33.135647Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-dynamics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:37.951927Z",
     "start_time": "2021-10-13T02:08:37.880120Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-portal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:41.333601Z",
     "start_time": "2021-10-13T02:08:41.320636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deep learning model layers\n",
    "def bulid_model_CNN():  \n",
    "    \n",
    "    model_list = []   \n",
    "    \n",
    "    cnn_out_1 = 20 #16\n",
    "    cnn_len_1 = 460 #20\n",
    "    \n",
    "    model_list.append(\n",
    "            layers.Conv1D(cnn_out_1, cnn_len_1, input_shape=(X_train.shape[1],1)),\n",
    "            ) \n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    model_list.append(layers.Activation('relu'))\n",
    "    model_list.append(layers.MaxPooling1D(3))    \n",
    "   \n",
    "    rnn_out = 16  \n",
    "    lr_rate = 1e-3\n",
    "    \n",
    "    \n",
    "    model_list.append(layers.Bidirectional(layers.LSTM(units=rnn_out, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    \n",
    "    model_list.append(layers.Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    model = models.Sequential(model_list)   \n",
    "    \n",
    "    \n",
    "    opt = RMSprop(lr=lr_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-amino",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:42.033115Z",
     "start_time": "2021-10-13T02:08:42.024140Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-killer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:43.386613Z",
     "start_time": "2021-10-13T02:08:43.360647Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-stadium",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:08:44.008253Z",
     "start_time": "2021-10-13T02:08:44.000238Z"
    }
   },
   "outputs": [],
   "source": [
    "# These callback function to schedule the learning rate and save weights of the model during training.\n",
    "filepath = \"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "callbacks_list = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10), \n",
    "                   ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='min')  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-nerve",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:19:47.516248Z",
     "start_time": "2021-10-13T02:08:45.017199Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 4  # k-fold cross validation\n",
    "num_val_samples = X_train.shape[0] //k\n",
    "num_epochs = 75\n",
    "all_scores = []\n",
    "all_scores_history = []\n",
    "train_loss_history_list = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    K.clear_session()  # clear previous wieghts to avoid cross-talk\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [X_train[:i * num_val_samples],\n",
    "         X_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)              \n",
    "    model = bulid_model_CNN()\n",
    "\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=64, verbose=0, validation_data=(val_data, val_targets),\n",
    "                        callbacks=callbacks_list)\n",
    "        \n",
    "    # Evaluate the model on the validation data\n",
    "    mae_history = history.history['val_loss']\n",
    "    train_loss_history = history.history['loss']    \n",
    "\n",
    "    all_scores_history.append(mae_history)\n",
    "    train_loss_history_list.append(train_loss_history)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-financing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:22:48.669499Z",
     "start_time": "2021-10-13T03:22:47.733586Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-upper",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:22:52.308688Z",
     "start_time": "2021-10-13T03:22:52.093148Z"
    }
   },
   "outputs": [],
   "source": [
    "averaged_history_val_loss = np.mean(np.array(all_scores_history[1:]), axis=0)\n",
    "averaged_history_train_loss = np.mean(np.array(train_loss_history_list[1:]), axis=0)\n",
    "font_size = 16\n",
    "# plot loss on training set and validation set \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(averaged_history_train_loss)\n",
    "plt.plot(averaged_history_val_loss)\n",
    "plt.title('Model loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.ylabel('Loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.xlabel('Epoch', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.legend(['Train', 'Validation'], loc='upper right', prop={'family':'Times New Roman', 'size':font_size})\n",
    "plt.xticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "#plt.savefig('loss.eps',dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-publication",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:22:57.932766Z",
     "start_time": "2021-10-13T03:22:57.922769Z"
    }
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-genome",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:22:59.613062Z",
     "start_time": "2021-10-13T03:22:59.380275Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-nurse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:24:38.714263Z",
     "start_time": "2021-10-13T03:24:38.700263Z"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(pres, y_test):\n",
    "    cm = np.zeros((8,8))\n",
    "    for id, pre in enumerate(pres):\n",
    "        column = int(pre[0]*4 + pre[1] * 2 + pre[2] * 1 )\n",
    "        row = int(y_test[id, 0]*4 + y_test[id, 1] * 2 + y_test[id, 2] * 1)\n",
    "        cm[row, column] += 1\n",
    "    return cm\n",
    "\n",
    "def heatmap(ma, filename):\n",
    "    labels = [\"000\", \"001\",\"010\",\"011\",\"100\",\"101\",\"110\",\"111\"]\n",
    "    acc = np.round(np.sum(np.diag(ma))/np.sum(ma), 5)\n",
    "    ma = pd.DataFrame(ma, index=labels, columns=labels)\n",
    "    f,ax = plt.subplots(figsize=(9, 6))\n",
    "    ax = sns.heatmap(ma, annot=True, center=11, cmap='RdYlBu')\n",
    "    plt.xlabel(\"Predicted labels\",fontdict={'size':16})\n",
    "    plt.ylabel(\"Target labels\",fontdict={'size':16})\n",
    "    plt.title('Accuracy = %s %%'%str(acc*100))\n",
    "    #plt.savefig('%s.eps'%str(filename),dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-liberal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:23:02.851193Z",
     "start_time": "2021-10-13T03:23:02.836243Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-frontier",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:26:01.539142Z",
     "start_time": "2021-10-13T03:25:52.572938Z"
    }
   },
   "outputs": [],
   "source": [
    "# The model_names may be different, please check the data/4_bins/weights/ folder\n",
    "models_names = ['weights-improvement-01-0.17.hdf5', 'weights-improvement-17-0.02.hdf5', 'weights-improvement-30-0.01.hdf5']\n",
    "for id, name in  enumerate(models_names):\n",
    "    model = load_model(name)\n",
    "    pre = model.predict(X_test)\n",
    "    cm = confusion_matrix(np.around(pre), y_test)\n",
    "    heatmap(cm, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-watch",
   "metadata": {},
   "source": [
    "# 20 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-reference",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-england",
   "metadata": {},
   "source": [
    "Note that before running each section, the kernel must be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-marble",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:01.128379Z",
     "start_time": "2021-10-13T03:30:01.084445Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../data/20_bins/20bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-fourth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:02.753694Z",
     "start_time": "2021-10-13T03:30:01.525996Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-couple",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:02.768582Z",
     "start_time": "2021-10-13T03:30:02.753694Z"
    }
   },
   "outputs": [],
   "source": [
    "max_min = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "cwd = os.getcwd()\n",
    "def prepare_data():    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)])):        \n",
    "        path = os.path.join(cwd, ''.join(i))        \n",
    "        names = os.listdir(path)       \n",
    "        \n",
    "        for name in names:            \n",
    "            X.append(pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[2]).apply(max_min).to_numpy())            \n",
    "            Y.append(np.array(eval('[' + ','.join(i) + ']')))            \n",
    "        \n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-validation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:07.013690Z",
     "start_time": "2021-10-13T03:30:03.723883Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-softball",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:07.029187Z",
     "start_time": "2021-10-13T03:30:07.013690Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-pasta",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:07.060284Z",
     "start_time": "2021-10-13T03:30:07.031161Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_d = np.zeros((X_train.shape[0]*2, X_train.shape[1],  X_train.shape[2]))\n",
    "y_train_d = np.zeros((y_train.shape[0]*2, y_train.shape[1]))\n",
    "\n",
    "for id  in range(X_train.shape[0]):\n",
    "    X_train_d[id] = X_train[id]\n",
    "    y_train_d[id] = y_train[id]\n",
    "    X_train_d[id + X_train.shape[0]] = max_min(X_train[id] + np.random.normal(loc=0, scale=0.1, size = (X_train.shape[1], X_train.shape[2])))\n",
    "    y_train_d[id + X_train.shape[0]] = y_train[id]\n",
    "\n",
    "X_train = X_train_d\n",
    "y_train = y_train_d\n",
    "del X_train_d\n",
    "del y_train_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-gibson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:07.076266Z",
     "start_time": "2021-10-13T03:30:07.061263Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-grenada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:09.022718Z",
     "start_time": "2021-10-13T03:30:07.077250Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-astronomy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:09.038602Z",
     "start_time": "2021-10-13T03:30:09.022718Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-gentleman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:09.054559Z",
     "start_time": "2021-10-13T03:30:09.039586Z"
    }
   },
   "outputs": [],
   "source": [
    "def bulid_model_CNN():  \n",
    "    \n",
    "    model_list = []   \n",
    "    \n",
    "    cnn_out_1 = 20 #16\n",
    "    cnn_len_1 = 460 #20\n",
    "    \n",
    "    model_list.append(\n",
    "            layers.Conv1D(cnn_out_1, cnn_len_1, input_shape=(X_train.shape[1],1)),\n",
    "            ) \n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    model_list.append(layers.Activation('relu'))\n",
    "    model_list.append(layers.MaxPooling1D(3))      \n",
    "    \n",
    "    rnn_out = 16  \n",
    "    lr_rate = 1e-3\n",
    "    \n",
    "    \n",
    "    model_list.append(layers.Bidirectional(layers.LSTM(units=rnn_out, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    \n",
    "    model_list.append(layers.Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    model = models.Sequential(model_list)   \n",
    "    \n",
    "    \n",
    "    opt = RMSprop(lr=lr_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-activation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:09.525351Z",
     "start_time": "2021-10-13T03:30:09.515378Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-hardware",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:10.720895Z",
     "start_time": "2021-10-13T03:30:10.691922Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-census",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:30:11.795634Z",
     "start_time": "2021-10-13T03:30:11.788604Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "callbacks_list = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10),\n",
    "                   ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='min')  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-township",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T03:40:49.233704Z",
     "start_time": "2021-10-13T03:30:14.284876Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_samples = X_train.shape[0] //k\n",
    "num_epochs = 75\n",
    "all_scores = []\n",
    "all_scores_history = []\n",
    "train_loss_history_list = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    K.clear_session()  # memory clean \n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [X_train[:i * num_val_samples],\n",
    "         X_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)              \n",
    "    model = bulid_model_CNN()\n",
    "\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=64, verbose=0, validation_data=(val_data, val_targets),\n",
    "                        callbacks=callbacks_list)\n",
    "        \n",
    "    # Evaluate the model on the validation data\n",
    "    mae_history = history.history['val_loss']\n",
    "    train_loss_history = history.history['loss']    \n",
    "\n",
    "    all_scores_history.append(mae_history)\n",
    "    train_loss_history_list.append(train_loss_history)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-candle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:49:54.237230Z",
     "start_time": "2021-09-18T05:49:53.956224Z"
    }
   },
   "outputs": [],
   "source": [
    "averaged_history_val_loss = np.mean(np.array(all_scores_history[1:]), axis=0)\n",
    "averaged_history_train_loss = np.mean(np.array(train_loss_history_list[1:]), axis=0)\n",
    "font_size = 16\n",
    "\n",
    "# plot loss on training set and validation set \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(averaged_history_train_loss)\n",
    "plt.plot(averaged_history_val_loss)\n",
    "plt.title('Model loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.ylabel('Loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.xlabel('Epoch', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.legend(['Train', 'Test'], loc='upper right', prop={'family':'Times New Roman', 'size':font_size})\n",
    "plt.xticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "#plt.savefig('loss.eps',dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-growth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:50:00.397302Z",
     "start_time": "2021-09-18T05:50:00.307976Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-excerpt",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:50:03.131523Z",
     "start_time": "2021-09-18T05:50:03.122548Z"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(pres, y_test):\n",
    "    cm = np.zeros((8,8))\n",
    "    for id, pre in enumerate(pres):\n",
    "        column = int(pre[0]*4 + pre[1] * 2 + pre[2] * 1 )\n",
    "        row = int(y_test[id, 0]*4 + y_test[id, 1] * 2 + y_test[id, 2] * 1)\n",
    "        cm[row, column] += 1\n",
    "    return cm\n",
    "\n",
    "def heatmap(ma, filename):\n",
    "    labels = [\"000\", \"001\",\"010\",\"011\",\"100\",\"101\",\"110\",\"111\"]\n",
    "    acc = np.round(np.sum(np.diag(ma))/np.sum(ma), 5)\n",
    "    ma = pd.DataFrame(ma, index=labels, columns=labels)\n",
    "    f,ax = plt.subplots(figsize=(9, 6))\n",
    "    ax = sns.heatmap(ma, annot=True, center=11, cmap='RdYlBu')\n",
    "    plt.xlabel(\"Predicted labels\",fontdict={'size':16})\n",
    "    plt.ylabel(\"Target labels\",fontdict={'size':16})\n",
    "    plt.title('Accuracy = %s %%'%str(acc*100))\n",
    "    #plt.savefig('%s.eps'%str(filename),dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-standard",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:50:04.181548Z",
     "start_time": "2021-09-18T05:50:04.178554Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-rehabilitation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:50:04.971533Z",
     "start_time": "2021-09-18T05:50:04.962560Z"
    }
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-motel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:02.038113Z",
     "start_time": "2021-09-18T05:50:51.634482Z"
    }
   },
   "outputs": [],
   "source": [
    "models_names = ['weights-improvement-01-0.25.hdf5','weights-improvement-58-0.03.hdf5','weights-improvement-64-0.02.hdf5']\n",
    "for id, name in  enumerate(models_names):\n",
    "    model = load_model(name)\n",
    "    pre = model.predict(X_test)\n",
    "    cm = confusion_matrix(np.around(pre), y_test)\n",
    "    heatmap(cm, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-banana",
   "metadata": {},
   "source": [
    "## Master equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-upgrade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T01:36:03.095488Z",
     "start_time": "2021-10-12T01:36:03.084548Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-navigation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T01:36:04.330535Z",
     "start_time": "2021-10-12T01:36:04.308561Z"
    }
   },
   "outputs": [],
   "source": [
    "def heatmap(ma, filename):\n",
    "    labels = [\"000\", \"001\",\"010\",\"011\",\"100\",\"101\",\"110\",\"111\"]\n",
    "    acc = np.round(np.sum(np.diag(ma))/np.sum(ma), 5)\n",
    "    ma = pd.DataFrame(ma, index=labels, columns=labels)\n",
    "    f,ax = plt.subplots(figsize=(9, 6))\n",
    "    ax = sns.heatmap(ma, annot=True, center=11, cmap='RdYlBu')\n",
    "    plt.xlabel(\"Predicted labels\",fontdict={'size':16})\n",
    "    plt.ylabel(\"Target labels\",fontdict={'size':16})\n",
    "    plt.title('Accuracy = %s %%'%str(acc*100))\n",
    "    #plt.savefig('%s.eps'%str(filename),dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-rally",
   "metadata": {},
   "source": [
    "The data below come from the results of Mathematica notebook \"/notebook/Fig5/20bins_fitting.nb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-holly",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T03:20:30.658220Z",
     "start_time": "2021-10-12T03:20:29.921878Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_master=np.concatenate([np.histogram([0,0,0,0,0,0,0,6,0,0,0,0,0,0,7,0,0,0,0,0],bins=np.arange(9))[0],\n",
    "np.histogram([0,0,0,1,4,4,4,0,0,0,0,1,4,0,0,0,0,0,0,7],bins=np.arange(9))[0],\n",
    "np.histogram([0,0,6,2,2,0,2,0,2,0,2,1,0,2,2,2,0,2,0,2],bins=np.arange(9))[0],\n",
    "np.histogram([0,0,0,0,1,0,0,5,2,0,0,0,0,0,0,0,0,0,0,0],bins=np.arange(9))[0],\n",
    "np.histogram([1,6,6,0,6,1,1,1,0,1,1,6,1,0,0,0,6,0,5,0],bins=np.arange(9))[0],\n",
    "np.histogram([0,0,0,0,5,5,2,0,0,0,0,0,2,0,0,2,2,2,0,2],bins=np.arange(9))[0],\n",
    "np.histogram([3,0,3,3,0,3,3,4,1,4,3,0,2,3,4,0,0,0,3,3],bins=np.arange(9))[0],\n",
    "np.histogram([0,0,0,0,7,0,0,0,3,0,5,0,0,0,0,0,0,0,1,0],bins=np.arange(9))[0]],axis=0).reshape(8,8)\n",
    "heatmap(cm_master,'20bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-senate",
   "metadata": {},
   "source": [
    "# 200 Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-section",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-intake",
   "metadata": {},
   "source": [
    "Note that before running each section, the kernel must be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-neighborhood",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:09.765208Z",
     "start_time": "2021-09-18T05:51:09.750466Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../data/resolution/resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-multimedia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:11.244847Z",
     "start_time": "2021-09-18T05:51:09.964591Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-advantage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:11.621130Z",
     "start_time": "2021-09-18T05:51:11.602139Z"
    }
   },
   "outputs": [],
   "source": [
    "max_min = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "cwd = os.getcwd()\n",
    "\n",
    "def prepare_data():    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)])):        \n",
    "        path = os.path.join(cwd, ''.join(i))        \n",
    "        names = os.listdir(path)       \n",
    "        \n",
    "        for name in names:            \n",
    "            X.append(pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[2]).apply(max_min).to_numpy())            \n",
    "            Y.append(np.array(eval('[' + ','.join(i) + ']')))            \n",
    "        \n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-bhutan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:16.304382Z",
     "start_time": "2021-09-18T05:51:12.636562Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-dominican",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:16.319352Z",
     "start_time": "2021-09-18T05:51:16.305381Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-patch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:16.350313Z",
     "start_time": "2021-09-18T05:51:16.321361Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_d = np.zeros((X_train.shape[0]*2, X_train.shape[1],  X_train.shape[2]))\n",
    "y_train_d = np.zeros((y_train.shape[0]*2, y_train.shape[1]))\n",
    "\n",
    "for id  in range(X_train.shape[0]):\n",
    "    X_train_d[id] = X_train[id]\n",
    "    y_train_d[id] = y_train[id]\n",
    "    X_train_d[id + X_train.shape[0]] = max_min(X_train[id] + np.random.normal(loc=0, scale=0.5, size = (X_train.shape[1], X_train.shape[2])))\n",
    "    y_train_d[id + X_train.shape[0]] = y_train[id]\n",
    "\n",
    "X_train = X_train_d\n",
    "y_train = y_train_d\n",
    "del X_train_d\n",
    "del y_train_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-round",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:16.366270Z",
     "start_time": "2021-09-18T05:51:16.352256Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-choice",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:18.393496Z",
     "start_time": "2021-09-18T05:51:16.367217Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-smith",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:18.408393Z",
     "start_time": "2021-09-18T05:51:18.395464Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-freeware",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:18.424555Z",
     "start_time": "2021-09-18T05:51:18.410397Z"
    }
   },
   "outputs": [],
   "source": [
    "def bulid_model_CNN():  \n",
    "    \n",
    "    model_list = []   \n",
    "    \n",
    "    cnn_out_1 = 20 #16\n",
    "    cnn_len_1 = 460 #20\n",
    "    \n",
    "    model_list.append(\n",
    "            layers.Conv1D(cnn_out_1, cnn_len_1, input_shape=(X_train.shape[1],1)),\n",
    "            ) \n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    model_list.append(layers.Activation('relu'))\n",
    "    model_list.append(layers.MaxPooling1D(3))  \n",
    "        \n",
    "    rnn_out = 16  \n",
    "    lr_rate = 1e-3\n",
    "    \n",
    "    \n",
    "    model_list.append(layers.Bidirectional(layers.LSTM(units=rnn_out, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    \n",
    "    model_list.append(layers.Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    model = models.Sequential(model_list)   \n",
    "    \n",
    "    \n",
    "    opt = RMSprop(lr=lr_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-works",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:19.123533Z",
     "start_time": "2021-09-18T05:51:19.115554Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-knitting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:20.084235Z",
     "start_time": "2021-09-18T05:51:20.074261Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-montreal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T05:51:21.437372Z",
     "start_time": "2021-09-18T05:51:21.419437Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "callbacks_list = [ ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10), \n",
    "                  ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='min') ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-nirvana",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T06:08:54.928501Z",
     "start_time": "2021-09-18T05:51:23.690572Z"
    }
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_samples = X_train.shape[0] //k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "all_scores_history = []\n",
    "train_loss_history_list = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    K.clear_session()  # memory clean\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [X_train[:i * num_val_samples],\n",
    "         X_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)              \n",
    "    model = bulid_model_CNN()\n",
    "\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=64, verbose=0, validation_data=(val_data, val_targets), callbacks=callbacks_list)\n",
    "        \n",
    "    # Evaluate the model on the validation data\n",
    "    mae_history = history.history['val_loss']\n",
    "    train_loss_history = history.history['loss']    \n",
    "\n",
    "    all_scores_history.append(mae_history)\n",
    "    train_loss_history_list.append(train_loss_history)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-skiing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T06:10:35.339929Z",
     "start_time": "2021-09-18T06:10:35.082100Z"
    }
   },
   "outputs": [],
   "source": [
    "averaged_history_val_loss = np.mean(np.array(all_scores_history[1:]), axis=0)\n",
    "averaged_history_train_loss = np.mean(np.array(train_loss_history_list[1:]), axis=0)\n",
    "font_size = 16\n",
    "\n",
    "# plot loss on training set and validation set \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(averaged_history_train_loss)\n",
    "plt.plot(averaged_history_val_loss)\n",
    "plt.title('Model loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.ylabel('Loss', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.xlabel('Epoch', fontdict={'family' : 'Times New Roman', 'size':font_size})\n",
    "plt.legend(['Train', 'Test'], loc='upper right', prop={'family':'Times New Roman', 'size':font_size})\n",
    "plt.xticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', fontsize=font_size)\n",
    "#plt.savefig('loss.eps',dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-belize",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T06:51:49.537095Z",
     "start_time": "2021-10-11T06:51:49.289562Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-cocktail",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T06:51:26.912510Z",
     "start_time": "2021-10-11T06:51:26.889572Z"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(pres, y_test):\n",
    "    cm = np.zeros((8,8))\n",
    "    for id, pre in enumerate(pres):\n",
    "        column = int(pre[0]*4 + pre[1] * 2 + pre[2] * 1 )\n",
    "        row = int(y_test[id, 0]*4 + y_test[id, 1] * 2 + y_test[id, 2] * 1)\n",
    "        cm[row, column] += 1\n",
    "    return cm\n",
    "\n",
    "def heatmap(ma, filename):\n",
    "    labels = [\"000\", \"001\",\"010\",\"011\",\"100\",\"101\",\"110\",\"111\"]\n",
    "    acc = np.round(np.sum(np.diag(ma))/np.sum(ma), 5)\n",
    "    ma = pd.DataFrame(ma, index=labels, columns=labels)\n",
    "    f,ax = plt.subplots(figsize=(9, 6))\n",
    "    ax = sns.heatmap(ma, annot=True, center=11, cmap='RdYlBu')\n",
    "    plt.xlabel(\"Predicted labels\",fontdict={'size':16})\n",
    "    plt.ylabel(\"Target labels\",fontdict={'size':16})\n",
    "    plt.title('Accuracy = %s %%'%str(acc*100))\n",
    "    #plt.savefig('%s.eps'%str(filename),dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-mileage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T06:10:43.311645Z",
     "start_time": "2021-09-18T06:10:43.306659Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-herald",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T06:11:59.315154Z",
     "start_time": "2021-09-18T06:11:49.697298Z"
    }
   },
   "outputs": [],
   "source": [
    "models_names = ['weights-improvement-13-0.11.hdf5', 'weights-improvement-57-0.02.hdf5', 'weights-improvement-100-0.03.hdf5']\n",
    "for id, name in  enumerate(models_names):\n",
    "    model = load_model(name)\n",
    "    pre = model.predict(X_test)\n",
    "    cm = confusion_matrix(np.around(pre), y_test)\n",
    "    heatmap(cm, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-binary",
   "metadata": {},
   "source": [
    "## Mster equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-stationery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T19:10:19.716403Z",
     "start_time": "2021-10-11T19:10:19.638338Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-eleven",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T01:38:23.075498Z",
     "start_time": "2021-10-12T01:38:23.051525Z"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(pres, y_test):\n",
    "    cm = np.zeros((8,8))\n",
    "    for id, pre in enumerate(pres):\n",
    "        column = int(pre[0]*4 + pre[1] * 2 + pre[2] * 1 )\n",
    "        row = int(y_test[id, 0]*4 + y_test[id, 1] * 2 + y_test[id, 2] * 1)\n",
    "        cm[row, column] += 1\n",
    "    return cm\n",
    "\n",
    "def heatmap(ma, filename):\n",
    "    labels = [\"000\", \"001\",\"010\",\"011\",\"100\",\"101\",\"110\",\"111\"]\n",
    "    acc = np.round(np.sum(np.diag(ma))/np.sum(ma), 5)\n",
    "    ma = pd.DataFrame(ma, index=labels, columns=labels)\n",
    "    f,ax = plt.subplots(figsize=(9, 6))\n",
    "    ax = sns.heatmap(ma, annot=True, center=11, cmap='RdYlBu')\n",
    "    plt.xlabel(\"Predicted labels\",fontdict={'size':16})\n",
    "    plt.ylabel(\"Target labels\",fontdict={'size':16})\n",
    "    plt.title('Accuracy = %s %%'%str(acc*100))\n",
    "    #plt.savefig('%s.eps'%str(filename),dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-spice",
   "metadata": {},
   "source": [
    "The data below come from the results of Mathematica notebook \"/notebook/Fig5/200MHz_fitting.nb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-three",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T03:25:37.226381Z",
     "start_time": "2021-10-12T03:25:36.498197Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_master=np.concatenate([np.histogram([0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0],bins=np.arange(9))[0],\n",
    "np.histogram([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 5, 1, 1, 1, 1],bins=np.arange(9))[0],\n",
    "np.histogram([6, 2, 2, 2, 2, 2, 1, 4, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 2],bins=np.arange(9))[0],\n",
    "np.histogram([7, 7, 3, 7, 7, 0, 3, 7, 0, 3, 3, 7, 3, 3, 3, 7, 3, 3, 3, 3],bins=np.arange(9))[0],\n",
    "np.histogram([0, 4, 0, 0, 4, 4, 4, 4, 0, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 4],bins=np.arange(9))[0],\n",
    "np.histogram([0, 5, 5, 1, 1, 0, 1, 1, 1, 1, 5, 3, 0, 0, 0, 0, 3, 1, 0, 5],bins=np.arange(9))[0],\n",
    "np.histogram([6, 6, 0, 0, 6, 6, 0, 6, 0, 6, 4, 6, 2, 6, 6, 6, 0, 6, 6, 7],bins=np.arange(9))[0],\n",
    "np.histogram([3, 7, 0, 7, 7, 0, 6, 7, 7, 3, 0, 0, 0, 0, 7, 2, 7, 1, 7, 0],bins=np.arange(9))[0]],axis=0).reshape(8,8)\n",
    "heatmap(cm_master,'200Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-width",
   "metadata": {},
   "source": [
    "# Data for additional noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-output",
   "metadata": {},
   "source": [
    "Note that before running each section, the kernel must be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-tracy",
   "metadata": {},
   "source": [
    "The code in this section is to prepare the data with white noise for the master equation fitting in `4bins_fitting.nb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-manitoba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T06:49:41.316594Z",
     "start_time": "2021-10-13T06:49:41.303587Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ..\\data\\additional_noise\\additional_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-compound",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T06:49:44.022028Z",
     "start_time": "2021-10-13T06:49:43.386448Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "scale_list = np.arange(0.05, 5.5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-amendment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T06:52:23.950855Z",
     "start_time": "2021-10-13T06:51:29.106865Z"
    }
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "cwd = os.getcwd()\n",
    "\n",
    "max_min = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "#np.set_printoptions(precision=3)\n",
    "\n",
    "for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)],str(0))):        \n",
    "    path = os.path.join(cwd, ''.join(i))        \n",
    "    names = os.listdir(path)       \n",
    "\n",
    "    for name in names: \n",
    "        t = pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[0]).to_numpy()\n",
    "        X = (pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[2]).apply(max_min).to_numpy())            \n",
    "        Y.append(np.array(eval('[' + ','.join(i) + ']')))   \n",
    "        \n",
    "        for scale in scale_list:\n",
    "            X_new = np.zeros_like(X)\n",
    "            X_new = (X + np.random.normal(loc=0, scale=scale, size = X.shape))\n",
    "            #np.savetxt(str(name)[:-4]+str(scale)+'.txt', np.c_[t ,X_new] ,delimiter='\\t', fmt='%.4e')\n",
    "            \n",
    "            try:\n",
    "                os.mkdir(''.join(i)+'/'+\"{0:.2f}\".format(scale))\n",
    "\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "            \n",
    "            np.savetxt(''.join(i)+'/'+\"{0:.2f}\".format(scale)+'/'+str(name)[:-4]+'.txt', np.c_[t ,max_min(X_new)] ,delimiter='\\t', fmt='%.4e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T06:51:15.334000Z",
     "start_time": "2021-10-13T06:51:11.746175Z"
    }
   },
   "outputs": [],
   "source": [
    "# codes to delete the signals with white noise, usually donot need to run.\n",
    "\"\"\"\n",
    "import shutil\n",
    "\n",
    "cwd = os.getcwd()\n",
    "for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)],str(0))):   \n",
    "    path = os.path.join(cwd, ''.join(i))  \n",
    "    for scale in scale_list:\n",
    "        path1 = os.path.join(path, \"{0:.2f}\".format(scale))\n",
    "        try:\n",
    "            shutil.rmtree(path1)\n",
    "        except (FileNotFoundError, PermissionError) as e:\n",
    "            print(\"Error: %s : %s\" % (path1, e.strerror))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-retro",
   "metadata": {},
   "source": [
    "# Additional Noises for 4 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-elder",
   "metadata": {},
   "source": [
    "Note that before running each section, the kernel must be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-picture",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T06:54:37.193273Z",
     "start_time": "2021-10-13T06:54:37.180267Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../data/4_bins/4bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-paper",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T04:24:34.700913Z",
     "start_time": "2021-10-11T04:24:31.473226Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 24\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks_list = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)] \n",
    "from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-london",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T04:24:34.716879Z",
     "start_time": "2021-10-11T04:24:34.701868Z"
    }
   },
   "outputs": [],
   "source": [
    "train_scale_list = np.arange(0.0, 3.5, 0.1) # The standard deviation of white noise in the training set\n",
    "model_list = []\n",
    "max_min = lambda x: (x-x.min())/(x.max()-x.min())\n",
    "cwd = os.getcwd()\n",
    "\n",
    "def prepare_data():    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in list(itertools.product([str(0),str(1)], [str(0),str(1)],[str(0),str(1)],[str(0)])):        \n",
    "        path = os.path.join(cwd, ''.join(i))        \n",
    "        names = os.listdir(path)       \n",
    "        \n",
    "        for name in names:            \n",
    "            X.append(pd.read_csv(os.path.join(path, name), sep='\\t',  usecols=[2]).apply(max_min).to_numpy())            \n",
    "            Y.append(np.array(eval('[' + ','.join(i) + ']')))            \n",
    "        \n",
    "    return np.array(X), np.array(Y)[:,:3]\n",
    "\n",
    "def bulid_model_CNN():  \n",
    "    \n",
    "    model_list = []   \n",
    "    \n",
    "    cnn_out_1 = 20 #16\n",
    "    cnn_len_1 = 460 #20\n",
    "    \n",
    "    model_list.append(\n",
    "            layers.Conv1D(cnn_out_1, cnn_len_1, input_shape=(X_train.shape[1],1)),\n",
    "            ) \n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    model_list.append(layers.Activation('relu'))\n",
    "    model_list.append(layers.MaxPooling1D(3))     \n",
    "    \n",
    "    rnn_out = 16  \n",
    "    lr_rate = 1e-3\n",
    "    \n",
    "    \n",
    "    model_list.append(layers.Bidirectional(layers.LSTM(units=rnn_out, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model_list.append(layers.BatchNormalization())\n",
    "    \n",
    "    model_list.append(layers.Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    model = models.Sequential(model_list)   \n",
    "    \n",
    "    \n",
    "    opt = RMSprop(lr=lr_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-assignment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T04:32:29.522578Z",
     "start_time": "2021-10-11T04:24:35.858045Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for scale in train_scale_list:\n",
    "    \n",
    "    X, Y = prepare_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=7)\n",
    "\n",
    "    X_train_d = np.zeros((X_train.shape[0]*2, X_train.shape[1],  X_train.shape[2]))\n",
    "    y_train_d = np.zeros((y_train.shape[0]*2, y_train.shape[1]))\n",
    "\n",
    "    for id  in range(X_train.shape[0]):\n",
    "        X_train_d[id] = X_train[id]\n",
    "        y_train_d[id] = y_train[id]\n",
    "        X_train_d[id + X_train.shape[0]] =(X_train[id] + np.random.normal(loc=0, scale=scale, size = (X_train.shape[1], X_train.shape[2])))\n",
    "        y_train_d[id + X_train.shape[0]] = y_train[id]\n",
    "\n",
    "    X_train = X_train_d\n",
    "    y_train = y_train_d\n",
    "    del X_train_d\n",
    "    del y_train_d\n",
    "\n",
    "    k = 4\n",
    "    num_val_samples = X_train.shape[0] //k\n",
    "    num_epochs = 75\n",
    "\n",
    "    for i in range(k):\n",
    "        print('processing fold #', i)\n",
    "        K.clear_session() \n",
    "        # Prepare the validation data: data from partition # k\n",
    "        val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        # Prepare the training data: data from all other partitions\n",
    "        partial_train_data = np.concatenate(\n",
    "            [X_train[:i * num_val_samples],\n",
    "             X_train[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [y_train[:i * num_val_samples],\n",
    "             y_train[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "                   \n",
    "        model = bulid_model_CNN()\n",
    "\n",
    "        history = model.fit(partial_train_data, partial_train_targets,\n",
    "                  epochs=num_epochs, batch_size=64, verbose=0, validation_data=(val_data, val_targets),\n",
    "                            callbacks=callbacks_list)\n",
    "\n",
    "    model.save(str(scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-waste",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T05:26:52.008480Z",
     "start_time": "2021-10-11T05:25:55.840551Z"
    }
   },
   "outputs": [],
   "source": [
    "scale_list=np.arange(0.0, 5.5, 0.1) # The standard deviation of white noise in the test set\n",
    "acc_mat = []\n",
    "for scale_train in train_scale_list:\n",
    "    acc_list = np.zeros((5, len(scale_list)))\n",
    "    reconstructed_model = models.load_model(str(scale_train))\n",
    "    for ii in range(5):\n",
    "\n",
    "        for ind, scale in enumerate( scale_list):\n",
    "            X_test_d = np.zeros((X_test.shape[0], X_test.shape[1],  X_test.shape[2]))\n",
    "            y_test_d = np.zeros((y_test.shape[0], y_test.shape[1]))\n",
    "\n",
    "            for id  in range(X_test.shape[0]):\n",
    "                X_test_d[id] = (X_test[id] + np.random.normal(loc=0, scale=scale, size = (X_test.shape[1], X_test.shape[2])))\n",
    "                y_test_d[id] = y_test[id] \n",
    "\n",
    "            mse, mae = reconstructed_model.evaluate(X_test_d, y_test_d, verbose=0)\n",
    "            pre = reconstructed_model.predict(X_test_d)\n",
    "            #display(np.abs(np.around(pre) -y_test))\n",
    "            accuracy = sum([1 if np.sum(np.abs(np.around(pre[i]) -y_test_d[i])) == 0 else 0 for i in range(len(pre)) ])/len(pre)\n",
    "            acc_list[ii,ind] = accuracy\n",
    "\n",
    "    acc_mat.append(np.mean(np.array(acc_list),axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-there",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T05:27:42.844978Z",
     "start_time": "2021-10-11T05:27:42.731283Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(acc_mat[0],'-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-stamp",
   "metadata": {},
   "source": [
    "Data is from master equation fitting, which is \"calculated by /notebook/4bins_fitting.nb\"\n",
    "\n",
    "The \"acc_mat0\" is \"acc_mat[0]\" calculated above. It can be replaced by \"acc_mat[0]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-increase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T02:53:39.538286Z",
     "start_time": "2021-10-09T02:53:39.216320Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 0, 3, 4, 4, 4, \\\n",
    "4, 4, 3, 5, 5, 5, 0, 6, 6, 6, 0, 0, 0, 1, 4, 0, 4, 0, 0, 0, 0, 0, 1, \\\n",
    "1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 7, 5, 7, \\\n",
    "3, 4, 3, 7, 7, 0, 6, 0, 7, 6, 4, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, \\\n",
    "2, 2, 2, 3, 3, 5, 3, 7, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 7, 7, 0, 6, \\\n",
    "7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 2, 3, 3, 3, \\\n",
    "3, 3, 4, 4, 4, 4, 4, 2, 5, 5, 0, 5, 6, 0, 7, 7, 6, 7, 7, 7, 7, 1, 0, \\\n",
    "0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 0, 0, 3, 3, 4, 4, 4, 4, \\\n",
    "4, 5, 2, 6, 0, 5, 6, 0, 0, 3, 0, 7, 7, 7, 7, 5, 0, 0, 0, 0, 0, 1, 1, \\\n",
    "6, 1, 0, 2, 2, 2, 0, 2, 3, 5, 3, 3, 3, 0, 0, 4, 0, 4, 0, 2, 6, 2, 4, \\\n",
    "0, 0, 0, 7, 6, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, \\\n",
    "0, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 3, 0, 0, 6, 1, 0, 1, 0, 0, 0, 7, \\\n",
    "7, 7, 7, 7, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 2, 4, 0, 0, 3, \\\n",
    "3, 0, 4, 4, 4, 4, 0, 2, 0, 7, 2, 0, 0, 2, 0, 0, 7, 7, 7, 7, 7, 0, 0, \\\n",
    "0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 0, 0, 0, 0, 3, 4, 0, 4, 4, 4, \\\n",
    "2, 2, 1, 3, 2, 6, 0, 1, 0, 6, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 1, 1, 1, \\\n",
    "1, 0, 2, 2, 2, 5, 0, 4, 0, 4, 3, 0, 0, 4, 4, 0, 4, 2, 2, 0, 0, 2, 0, \\\n",
    "0, 0, 1, 0, 7, 7, 5, 7, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, \\\n",
    "0, 3, 3, 0, 0, 0, 4, 4, 0, 4, 4, 2, 0, 0, 0, 2, 0, 0, 6, 0, 0, 7, 7, \\\n",
    "0, 4, 0, 4, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 0, 2, 0, 4, 0, 1, 4, \\\n",
    "6, 0, 0, 4, 4, 7, 0, 2, 0, 2, 6, 0, 7, 4, 6, 0, 0, 7, 7, 7, 0, 0, 0, \\\n",
    "0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 2, 0, 4, 0, 0, 3, 4, 4, 4, 0, 4, 0, \\\n",
    "0, 4, 1, 0, 0, 0, 0, 1, 0, 0, 3, 0, 7, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, \\\n",
    "0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 0, 4, 4, 4, 4, 4, 2, 2, 0, 0, 0, 6, 0, \\\n",
    "0, 0, 0, 1, 1, 0, 7, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 2, 2, 2, 0, 2, \\\n",
    "0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 7, 0, 0, 0, 0, 0, 0, 1, 6, 0, 1, 6, \\\n",
    "0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 4, 0, 3, 0, 0, 0, \\\n",
    "0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 6, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, \\\n",
    "0, 1, 0, 1, 4, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 4, 4, 0, 2, \\\n",
    "0, 0, 0, 0, 0, 0, 0, 6, 7, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 3, 1, 0, \\\n",
    "2, 0, 2, 2, 3, 0, 4, 4, 0, 3, 4, 0, 4, 4, 0, 0, 1, 0, 0, 6, 0, 0, 0, \\\n",
    "3, 1, 5, 5, 2, 2, 4, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 0, 2, 2, 0, \\\n",
    "0, 0, 0, 0, 0, 6, 0, 0, 4, 7, 6, 0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 0, 1, \\\n",
    "0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2, 2, 0, 0, 7, 0, 0, 0, 0, 0, 0, \\\n",
    "0, 0, 4, 0, 2, 0, 0, 0, 6, 0, 1, 0, 6, 0, 1, 6, 2, 0, 0, 4, 0, 4, 0, \\\n",
    "0, 0, 4, 1, 4, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 1, 0, 0, \\\n",
    "0, 0, 0, 1, 0, 0, 6, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, \\\n",
    "0, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 4, 4, 4, 2, 6, 0, 0, 0, 6, 1, 6, 0, \\\n",
    "0, 0, 0, 0, 4, 1, 4, 0, 0, 0, 0, 0, 0, 7, 3, 1, 2, 2, 2, 2, 6, 0, 6, \\\n",
    "0, 4, 4, 0, 0, 4, 0, 4, 2, 0, 0, 0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 4, 0, \\\n",
    "0, 0, 0, 0, 0, 1, 3, 1, 3, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 4, 4, 4, \\\n",
    "0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 4, 0, 0, 1, \\\n",
    "0, 1, 1, 0, 0, 2, 2, 5, 3, 0, 0, 0, 4, 0, 4, 0, 4, 0, 4, 3, 5, 0, 0, \\\n",
    "2, 0, 0, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 4, 1, 1, 0, 2, 1, 0, 2, \\\n",
    "2, 0, 0, 0, 0, 0, 0, 1, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, \\\n",
    "2, 1, 0, 0, 0, 0, 0, 4, 0, 0, 1, 1, 0, 1, 0, 0, 2, 2, 0, 2, 0, 3, 0, \\\n",
    "3, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 1, 6, 0, 0, 0, 6, 2, 0, 5, 0, 0, 0, \\\n",
    "0, 0, 0, 0, 1, 0, 1, 4, 0, 6, 2, 0, 0, 2, 0, 0, 0, 0, 1, 0, 4, 4, 0, \\\n",
    "0, 0, 0, 0, 0, 7, 0, 1, 6, 0, 6, 1, 1, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, \\\n",
    "7, 1, 1, 0, 2, 2, 2, 2, 0, 3, 3, 0, 0, 2, 4, 4, 0, 0, 0, 0, 0, 4, 4, \\\n",
    "6, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 2, 0, 0, \\\n",
    "5, 0, 0, 0, 2, 5, 6, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 3, 0, 0, \\\n",
    "0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 6, 0, 0, 0, 0, 0, 3, 0, \\\n",
    "0, 0, 4, 4, 5, 4, 1, 5, 1, 0, 5, 1, 2, 1, 0, 6, 0, 1, 0, 0, 0, 0, 0, \\\n",
    "0, 0, 0, 0, 1, 1, 3, 1, 2, 2, 0, 0, 4, 5, 0, 3, 0, 3, 0, 0, 4, 4, 4, \\\n",
    "5, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 0, 1, \\\n",
    "0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 5, 0, 0, 0, 0, 6, \\\n",
    "0, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 3, 0, \\\n",
    "0, 0, 0, 0, 0, 3, 0, 2, 2, 4, 4, 0, 5, 1, 0, 0, 0, 0, 4, 5, 0, 6, 0, \\\n",
    "0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 1, 4, 0, 1, 0, 2, 0, 4, 0, 0, 7, 0, \\\n",
    "0, 4, 4, 4, 4, 0, 0, 1, 1, 0, 0, 0, 0, 6, 6, 0, 0, 4, 0, 6, 0, 0, 0, \\\n",
    "0, 0, 0, 1, 1, 0, 1, 6, 0, 3, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, \\\n",
    "0, 0, 5, 0, 2, 0, 0, 0, 6, 0, 5, 0, 5, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, \\\n",
    "0, 0, 0, 4, 0, 0, 0, 3, 7, 0, 1, 0, 2, 4, 4, 0, 0, 1, 0, 0, 0, 4, 0, \\\n",
    "4, 0, 2, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 2, 1, 1, 0, 2, 0, 0, 0, \\\n",
    "0, 0, 7, 5, 0, 0, 0, 4, 0, 6, 1, 1, 4, 0, 0, 1, 0, 0, 1, 0, 6, 7, 0, \\\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 0, 0, 0, 2, 2, 0, 4, 0, 0, 0, 0, 2, \\\n",
    "0, 4, 0, 0, 0, 0, 3, 0, 0, 2, 1, 6, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, \\\n",
    "0, 1, 0, 7, 0, 1, 6, 4, 2, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 4, \\\n",
    "0, 1, 0, 0, 0, 6, 1, 0, 0, 4, 0, 3, 0, 0, 0, 4, 0, 0, 0, 0, 1, 1, 3, \\\n",
    "6, 2, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 4, 1, 0, 0, 5, 0, 6, 0, 0, \\\n",
    "7, 0, 0, 1, 0, 0, 1, 0, 0, 4, 0, 0, 3, 0, 1, 0, 0, 0, 0, 3, 0, 0, 3, \\\n",
    "0, 0, 3, 0, 4, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 1, 0, 6, 4, 0, 0, 0, \\\n",
    "0, 0, 0, 0, 6, 0, 1, 0, 1, 0, 0, 2, 3, 0, 0, 0, 3, 0, 2, 0, 0, 0, 6, \\\n",
    "0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 6, 0, 1, 0, 0, 0, 0, 0, \\\n",
    "0, 1, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 5, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, \\\n",
    "0, 0, 4, 0, 0, 6, 0, 0, 2, 0, 0, 0, 4, 3, 0, 4, 0, 0, 0, 1, 1, 1, 2, \\\n",
    "0, 0, 0, 1, 0, 0, 0, 3, 2, 0, 0, 0, 4, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, \\\n",
    "0, 0, 7, 1, 2, 0, 4, 0, 0, 4, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, \\\n",
    "1, 0, 3, 0, 4, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 5, 4, 4, 0, 0, 0, 4, 0, \\\n",
    "2, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 1, 4, 0, \\\n",
    "0, 2, 5, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 1, 0, 0, 0, 0, 4, 4, 0, 0, 0, \\\n",
    "0, 0, 1, 0, 0, 0, 2, 0, 0, 7, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 3, 0, 3, \\\n",
    "0, 0, 2, 0, 4, 2, 1, 1, 0, 0, 0, 2, 2, 2, 0, 0, 3, 0, 1, 3, 6, 0, 2, \\\n",
    "2, 2, 2, 0, 0, 0, 3, 0, 0, 4, 2, 0, 0, 2, 6, 0, 2, 0, 2, 0, 0, 0, 0, \\\n",
    "0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 5, 4, 0, 1, 0, 2, 0, 0, 0, 3, 0, 4, 0, \\\n",
    "2, 0, 0, 0, 2, 4, 0, 0, 1, 1, 4, 1, 0, 0, 2, 6, 0, 3, 3, 0, 0, 0, 0, \\\n",
    "0, 0, 4, 0, 2, 0, 2, 6, 1, 0, 0, 0, 0, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, \\\n",
    "0, 5, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, \\\n",
    "0, 1, 0, 3, 0, 0, 2, 0, 4, 3, 0, 4, 0, 0, 4, 0, 4, 4, 0, 1, 5, 0, 0, \\\n",
    "2, 0, 6, 0, 6, 1, 0, 2, 4, 0, 0, 0, 0, 0, 2, 0, 1, 3, 1, 0, 2, 0, 0, \\\n",
    "0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 5, 1, 5, 0, 4, 0, 0, 0, 1, 0, \\\n",
    "3, 0, 0, 0, 6, 0, 0, 4, 0, 0, 3, 0, 2, 0, 4, 0, 0, 6, 3, 2, 0, 3, 0, \\\n",
    "0, 0, 4, 0, 3, 7, 1, 0, 4, 4, 0, 0, 0, 2, 0, 2, 0, 0, 5, 4, 0, 0, 4, \\\n",
    "0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 4, 0, 6, 0, \\\n",
    "0, 0, 0, 0, 1, 4, 0, 0, 3, 2, 0, 0, 0, 4, 3])\n",
    "\n",
    "acc_mat0 = np.array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "       1.        , 1.        , 1.        , 0.99473684, 0.99473684,\n",
    "       0.97368421, 0.98421053, 0.96315789, 0.92631579, 0.91052632,\n",
    "       0.9       , 0.85263158, 0.85789474, 0.78947368, 0.77894737,\n",
    "       0.76315789, 0.69473684, 0.68947368, 0.71052632, 0.67894737,\n",
    "       0.71578947, 0.61578947, 0.65263158, 0.59473684, 0.58421053,\n",
    "       0.54210526, 0.51578947, 0.56315789, 0.46842105, 0.52631579,\n",
    "       0.44736842, 0.47894737, 0.46842105, 0.49473684, 0.44210526,\n",
    "       0.38421053, 0.42631579, 0.39473684, 0.41052632, 0.39473684,\n",
    "       0.40526316, 0.41052632, 0.33684211, 0.35263158, 0.44210526,\n",
    "       0.32631579, 0.38947368, 0.31578947, 0.3       , 0.37368421])\n",
    "\n",
    "acc_list = np.zeros(len(data)//40)\n",
    "for j in range(len(data)//40):\n",
    "    acc = []\n",
    "    for i in range(8):\n",
    "        acc.append(len(np.where(data[(40*j+5*i): (40*j+5*i+5)]== i)[0]))\n",
    "    acc_list[j]=sum(acc)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(np.arange(0.05, 5.5, 0.1), acc_list/40,'-o', label='Master equation')\n",
    "plt.plot(np.arange(0, 5.5, 0.1), acc_mat0, '-o', label='Deep learning')\n",
    "plt.xlabel('Test set standard deviation ')\n",
    "plt.ylabel('Test set accuracy')\n",
    "plt.legend()\n",
    "#plt.savefig('testacc.eps',dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-tournament",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T17:52:33.869331Z",
     "start_time": "2021-10-09T17:52:33.623402Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_mat = np.squeeze(np.array(acc_mat))\n",
    "\n",
    "x = np.arange(0, 5.5, 0.1)\n",
    "y = np.arange(0, 3.5, 0.1)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.rcParams.update({\"font.size\":20})\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.pcolor(X, Y, acc_mat, shading='auto',edgecolors='None',snap=False)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Test set standard deviation ')\n",
    "plt.ylabel('Training set standard deviation')\n",
    "#plt.savefig('noise_map1.eps')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
